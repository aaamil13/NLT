"""
–ú–æ–¥—É–ª –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –¥–∞–Ω–Ω–∏ –∑–∞ BAO –∏ CMB –∞–Ω–∞–ª–∏–∑

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç–∏:
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Ä–µ–∞–ª–Ω–∏ BAO –¥–∞–Ω–Ω–∏
- –§–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ –∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª–∞—Ü–∏—è –Ω–∞ CMB –¥–∞–Ω–Ω–∏
- –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –≥—Ä–µ—à–∫–∏
- –ö–æ—Ä–µ–ª–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
- –ö–∞—á–µ—Å—Ç–≤–µ–Ω –∫–æ–Ω—Ç—Ä–æ–ª –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ
"""

import numpy as np
from scipy import interpolate, stats
from typing import Dict, List, Tuple, Any, Optional
import logging

logger = logging.getLogger(__name__)

class DataValidator:
    """–í–∞–ª–∏–¥–∏—Ä–∞–Ω–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ—Ç–æ –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ"""
    
    @staticmethod
    def validate_z_data(z: np.ndarray, z_min: float = 0.0, z_max: float = 10.0) -> bool:
        """
        –í–∞–ª–∏–¥–∏—Ä–∞ –¥–∞–Ω–Ω–∏ –∑–∞ —á–µ—Ä–≤–µ–Ω–æ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–µ
        
        Args:
            z: –ú–∞—Å–∏–≤ —Å —á–µ—Ä–≤–µ–Ω–∏ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–∏—è
            z_min: –ú–∏–Ω–∏–º–∞–ª–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ z
            z_max: –ú–∞–∫—Å–∏–º–∞–ª–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ z
            
        Returns:
            True –∞–∫–æ –¥–∞–Ω–Ω–∏—Ç–µ —Å–∞ –≤–∞–ª–∏–¥–Ω–∏
        """
        if not isinstance(z, np.ndarray):
            z = np.array(z)
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        checks = [
            len(z) > 0,  # –ù–µ –µ –ø—Ä–∞–∑–µ–Ω
            np.all(np.isfinite(z)),  # –ù—è–º–∞ NaN/inf
            np.all(z >= z_min),  # –í –¥–æ–ø—É—Å—Ç–∏–º–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω
            np.all(z <= z_max),
            np.all(z >= 0)  # –§–∏–∑–∏—á–Ω–æ —Ä–∞–∑—É–º–Ω–æ
        ]
        
        return all(checks)
    
    @staticmethod
    def validate_measurement_data(data: np.ndarray, errors: np.ndarray) -> bool:
        """
        –í–∞–ª–∏–¥–∏—Ä–∞ –∏–∑–º–µ—Ä–∏—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏ —Å –≥—Ä–µ—à–∫–∏
        
        Args:
            data: –ò–∑–º–µ—Ä–µ–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            errors: –ì—Ä–µ—à–∫–∏ –≤ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è—Ç–∞
            
        Returns:
            True –∞–∫–æ –¥–∞–Ω–Ω–∏—Ç–µ —Å–∞ –≤–∞–ª–∏–¥–Ω–∏
        """
        if not isinstance(data, np.ndarray):
            data = np.array(data)
        if not isinstance(errors, np.ndarray):
            errors = np.array(errors)
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        checks = [
            len(data) == len(errors),  # –ï–¥–Ω–∞–∫–≤–∏ —Ä–∞–∑–º–µ—Ä–∏
            np.all(np.isfinite(data)),  # –ù—è–º–∞ NaN/inf
            np.all(np.isfinite(errors)),
            np.all(errors > 0),  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–∏ –≥—Ä–µ—à–∫–∏
            np.all(errors < 10 * np.abs(data))  # –†–∞–∑—É–º–Ω–∏ –≥—Ä–µ—à–∫–∏
        ]
        
        return all(checks)
    
    @staticmethod
    def detect_outliers(data: np.ndarray, threshold: float = 3.0) -> np.ndarray:
        """
        –û—Ç–∫—Ä–∏–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (outliers)
        
        Args:
            data: –î–∞–Ω–Ω–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑
            threshold: –ü—Ä–∞–≥ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            
        Returns:
            –ë—É–ª–µ–≤ –º–∞—Å–∏–≤ —Å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è—Ç–∞
        """
        if len(data) < 3:
            return np.zeros(len(data), dtype=bool)
            
        # Z-score –º–µ—Ç–æ–¥
        z_scores = np.abs(stats.zscore(data))
        outliers = z_scores > threshold
        
        logger.info(f"–û—Ç–∫—Ä–∏—Ç–∏ {np.sum(outliers)} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç {len(data)} —Ç–æ—á–∫–∏")
        return outliers


class BAODataProcessor:
    """–ü—Ä–æ—Ü–µ—Å–æ—Ä –∑–∞ BAO –¥–∞–Ω–Ω–∏"""
    
    def __init__(self):
        self.validator = DataValidator()
        
    def process_bao_measurements(self, z: np.ndarray, D_V_over_rs: np.ndarray, 
                               errors: np.ndarray, filter_outliers: bool = True) -> Dict[str, np.ndarray]:
        """
        –û–±—Ä–∞–±–æ—Ç–≤–∞ BAO –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è
        
        Args:
            z: –ß–µ—Ä–≤–µ–Ω–∏ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–∏—è
            D_V_over_rs: –û—Ç–Ω–æ—à–µ–Ω–∏–µ D_V/r_s
            errors: –ì—Ä–µ—à–∫–∏ –≤ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è—Ç–∞
            filter_outliers: –î–∞–ª–∏ –¥–∞ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è—Ç–∞
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        # –í–∞–ª–∏–¥–∏—Ä–∞–Ω–µ
        if not self.validator.validate_z_data(z):
            raise ValueError("–ù–µ–≤–∞–ª–∏–¥–Ω–∏ –¥–∞–Ω–Ω–∏ –∑–∞ —á–µ—Ä–≤–µ–Ω–æ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–µ")
        if not self.validator.validate_measurement_data(D_V_over_rs, errors):
            raise ValueError("–ù–µ–≤–∞–ª–∏–¥–Ω–∏ –∏–∑–º–µ—Ä–∏—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏")
            
        # –ö–æ–ø–∏—Ä–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ
        z_clean = z.copy()
        D_V_clean = D_V_over_rs.copy()
        err_clean = errors.copy()
        
        # –§–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        if filter_outliers:
            outliers = self.validator.detect_outliers(D_V_over_rs)
            if np.any(outliers):
                logger.warning(f"–ü—Ä–µ–º–∞—Ö–Ω–∞—Ç–∏ {np.sum(outliers)} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è")
                mask = ~outliers
                z_clean = z_clean[mask]
                D_V_clean = D_V_clean[mask]
                err_clean = err_clean[mask]
        
        # –°–æ—Ä—Ç–∏—Ä–∞–Ω–µ –ø–æ z
        sort_idx = np.argsort(z_clean)
        z_clean = z_clean[sort_idx]
        D_V_clean = D_V_clean[sort_idx]
        err_clean = err_clean[sort_idx]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_info = {
            'N_points': len(z_clean),
            'z_range': [z_clean.min(), z_clean.max()],
            'D_V_range': [D_V_clean.min(), D_V_clean.max()],
            'mean_error': np.mean(err_clean),
            'relative_error': np.mean(err_clean / D_V_clean)
        }
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ BAO –¥–∞–Ω–Ω–∏: {stats_info['N_points']} —Ç–æ—á–∫–∏")
        logger.info(f"Z –¥–∏–∞–ø–∞–∑–æ–Ω: {stats_info['z_range'][0]:.3f} - {stats_info['z_range'][1]:.3f}")
        
        return {
            'z': z_clean,
            'D_V_over_rs': D_V_clean,
            'errors': err_clean,
            'statistics': stats_info
        }
    
    def interpolate_bao_data(self, z: np.ndarray, D_V_over_rs: np.ndarray, 
                           errors: np.ndarray, z_target: np.ndarray) -> Dict[str, np.ndarray]:
        """
        –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–∞ BAO –¥–∞–Ω–Ω–∏ –Ω–∞ –Ω–æ–≤–∞ –º—Ä–µ–∂–∞
        
        Args:
            z: –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏ —á–µ—Ä–≤–µ–Ω–∏ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–∏—è
            D_V_over_rs: –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            errors: –ì—Ä–µ—à–∫–∏
            z_target: –¶–µ–ª–µ–≤–∞ –º—Ä–µ–∂–∞
            
        Returns:
            –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–∞–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∞—Ü–∏—è –Ω–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏—Ç–µ
        f_interp = interpolate.interp1d(z, D_V_over_rs, kind='cubic', 
                                      bounds_error=False, fill_value='extrapolate')
        D_V_interp = f_interp(z_target)
        
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∞—Ü–∏—è –Ω–∞ –≥—Ä–µ—à–∫–∏—Ç–µ
        f_err = interpolate.interp1d(z, errors, kind='linear',
                                   bounds_error=False, fill_value='extrapolate')
        err_interp = f_err(z_target)
        
        # –ú–∞—Å–∫–∏—Ä–∞–Ω–µ –Ω–∞ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–∞–Ω–∏—Ç–µ –æ–±–ª–∞—Å—Ç–∏
        mask = (z_target >= z.min()) & (z_target <= z.max())
        
        logger.info(f"–ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–∞–Ω–∏ {np.sum(mask)} –æ—Ç {len(z_target)} —Ç–æ—á–∫–∏")
        
        return {
            'z': z_target,
            'D_V_over_rs': D_V_interp,
            'errors': err_interp,
            'interpolation_mask': mask
        }
    
    def create_covariance_matrix(self, errors: np.ndarray, 
                               correlation_length: float = 0.1) -> np.ndarray:
        """
        –°—ä–∑–¥–∞–≤–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ BAO –¥–∞–Ω–Ω–∏
        
        Args:
            errors: –ì—Ä–µ—à–∫–∏ –≤ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è—Ç–∞
            correlation_length: –î—ä–ª–∂–∏–Ω–∞ –Ω–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏—è—Ç–∞ –≤ z
            
        Returns:
            –ö–æ—Ä–µ–ª–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
        """
        N = len(errors)
        cov_matrix = np.zeros((N, N))
        
        # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∏ –µ–ª–µ–º–µ–Ω—Ç–∏ (–¥–∏—Å–ø–µ—Ä—Å–∏–∏)
        np.fill_diagonal(cov_matrix, errors**2)
        
        # –ò–∑–≤—ä–Ω-–¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∏ –µ–ª–µ–º–µ–Ω—Ç–∏ (–∫–æ—Ä–µ–ª–∞—Ü–∏–∏)
        for i in range(N):
            for j in range(i+1, N):
                correlation = np.exp(-abs(i-j) / correlation_length)
                cov_matrix[i, j] = correlation * errors[i] * errors[j]
                cov_matrix[j, i] = cov_matrix[i, j]
        
        return cov_matrix


class CMBDataProcessor:
    """–ü—Ä–æ—Ü–µ—Å–æ—Ä –∑–∞ CMB –¥–∞–Ω–Ω–∏"""
    
    def __init__(self):
        self.validator = DataValidator()
        
    def process_cmb_power_spectrum(self, l: np.ndarray, C_l: np.ndarray, 
                                 C_l_err: np.ndarray, l_min: int = 2, 
                                 l_max: int = 2500) -> Dict[str, np.ndarray]:
        """
        –û–±—Ä–∞–±–æ—Ç–≤–∞ CMB power spectrum
        
        Args:
            l: –ú—É–ª—Ç–∏–ø–æ–ª–Ω–∏ –º–æ–º–µ–Ω—Ç–∏
            C_l: Power spectrum —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            C_l_err: –ì—Ä–µ—à–∫–∏ –≤ power spectrum
            l_min: –ú–∏–Ω–∏–º–∞–ª–µ–Ω l
            l_max: –ú–∞–∫—Å–∏–º–∞–ª–µ–Ω l
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        # –í–∞–ª–∏–¥–∏—Ä–∞–Ω–µ
        if not self.validator.validate_measurement_data(C_l, C_l_err):
            raise ValueError("–ù–µ–≤–∞–ª–∏–¥–Ω–∏ CMB –¥–∞–Ω–Ω–∏")
            
        # –§–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ –ø–æ l –¥–∏–∞–ø–∞–∑–æ–Ω
        mask = (l >= l_min) & (l <= l_max) & (C_l > 0)
        
        l_clean = l[mask]
        C_l_clean = C_l[mask]
        C_l_err_clean = C_l_err[mask]
        
        # –°–æ—Ä—Ç–∏—Ä–∞–Ω–µ –ø–æ l
        sort_idx = np.argsort(l_clean)
        l_clean = l_clean[sort_idx]
        C_l_clean = C_l_clean[sort_idx]
        C_l_err_clean = C_l_err_clean[sort_idx]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_info = {
            'N_points': len(l_clean),
            'l_range': [l_clean.min(), l_clean.max()],
            'C_l_range': [C_l_clean.min(), C_l_clean.max()],
            'mean_relative_error': np.mean(C_l_err_clean / C_l_clean)
        }
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ CMB –¥–∞–Ω–Ω–∏: {stats_info['N_points']} —Ç–æ—á–∫–∏")
        logger.info(f"l –¥–∏–∞–ø–∞–∑–æ–Ω: {stats_info['l_range'][0]} - {stats_info['l_range'][1]}")
        
        return {
            'l': l_clean,
            'C_l': C_l_clean,
            'C_l_err': C_l_err_clean,
            'statistics': stats_info
        }
    
    def extract_acoustic_peaks(self, l: np.ndarray, C_l: np.ndarray, 
                             n_peaks: int = 5) -> Dict[str, np.ndarray]:
        """
        –ò–∑–≤–ª–∏—á–∞ –∞–∫—É—Å—Ç–∏—á–Ω–∏ –ø–∏–∫–æ–≤–µ –æ—Ç CMB power spectrum
        
        Args:
            l: –ú—É–ª—Ç–∏–ø–æ–ª–Ω–∏ –º–æ–º–µ–Ω—Ç–∏
            C_l: Power spectrum —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            n_peaks: –ë—Ä–æ–π –ø–∏–∫–æ–≤–µ –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ
            
        Returns:
            –î–∞–Ω–Ω–∏ –∑–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        """
        # –°–º—É—Ç–∏—Ä–∞–Ω–µ –∑–∞ –Ω–∞–º–∞–ª—è–≤–∞–Ω–µ –Ω–∞ —à—É–º–∞
        from scipy.signal import savgol_filter
        C_l_smooth = savgol_filter(C_l, window_length=11, polyorder=3)
        
        # –ù–∞–º–∏—Ä–∞–Ω–µ –Ω–∞ –ø–∏–∫–æ–≤–µ
        from scipy.signal import find_peaks
        peaks, properties = find_peaks(C_l_smooth, height=0.1*C_l_smooth.max(), 
                                     distance=50)
        
        # –°–æ—Ä—Ç–∏—Ä–∞–Ω–µ –ø–æ –≤–∏—Å–æ—á–∏–Ω–∞
        peak_heights = C_l_smooth[peaks]
        sort_idx = np.argsort(peak_heights)[::-1]
        
        # –í–∑–∏–º–∞–Ω–µ –Ω–∞ –Ω–∞–π-–≤–∏—Å–æ–∫–∏—Ç–µ –ø–∏–∫–æ–≤–µ
        n_peaks = min(n_peaks, len(peaks))
        best_peaks = peaks[sort_idx[:n_peaks]]
        
        # –°–æ—Ä—Ç–∏—Ä–∞–Ω–µ –ø–æ l
        best_peaks = np.sort(best_peaks)
        
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏ {n_peaks} –∞–∫—É—Å—Ç–∏—á–Ω–∏ –ø–∏–∫–∞")
        
        return {
            'l_peaks': l[best_peaks],
            'C_l_peaks': C_l[best_peaks],
            'peak_indices': best_peaks,
            'n_peaks': n_peaks
        }
    
    def binning_cmb_data(self, l: np.ndarray, C_l: np.ndarray, 
                        C_l_err: np.ndarray, bin_size: int = 10) -> Dict[str, np.ndarray]:
        """
        –ì—Ä—É–ø–∏—Ä–∞ CMB –¥–∞–Ω–Ω–∏ –≤ bins –∑–∞ –Ω–∞–º–∞–ª—è–≤–∞–Ω–µ –Ω–∞ —à—É–º–∞
        
        Args:
            l: –ú—É–ª—Ç–∏–ø–æ–ª–Ω–∏ –º–æ–º–µ–Ω—Ç–∏
            C_l: Power spectrum —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            C_l_err: –ì—Ä–µ—à–∫–∏
            bin_size: –†–∞–∑–º–µ—Ä –Ω–∞ bin
            
        Returns:
            –ì—Ä—É–ø–∏—Ä–∞–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        n_bins = len(l) // bin_size
        
        l_binned = np.zeros(n_bins)
        C_l_binned = np.zeros(n_bins)
        C_l_err_binned = np.zeros(n_bins)
        
        for i in range(n_bins):
            start = i * bin_size
            end = (i + 1) * bin_size
            
            # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            l_binned[i] = np.mean(l[start:end])
            
            # –ü—Ä–µ—Ç–µ–≥–ª–µ–Ω–æ —É—Å—Ä–µ–¥–Ω—è–≤–∞–Ω–µ
            weights = 1 / C_l_err[start:end]**2
            C_l_binned[i] = np.average(C_l[start:end], weights=weights)
            
            # –ì—Ä–µ—à–∫–∞ –æ—Ç –ø—Ä–µ—Ç–µ–≥–ª–µ–Ω–æ —É—Å—Ä–µ–¥–Ω—è–≤–∞–Ω–µ
            C_l_err_binned[i] = 1 / np.sqrt(np.sum(weights))
        
        logger.info(f"–ì—Ä—É–ø–∏—Ä–∞–Ω–∏ –¥–∞–Ω–Ω–∏: {len(l)} -> {n_bins} bins")
        
        return {
            'l': l_binned,
            'C_l': C_l_binned,
            'C_l_err': C_l_err_binned,
            'bin_size': bin_size
        }


class StatisticalAnalyzer:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞ –¥–∞–Ω–Ω–∏"""
    
    @staticmethod
    def calculate_chi_squared(theory: np.ndarray, data: np.ndarray, 
                            errors: np.ndarray) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ œá¬≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        
        Args:
            theory: –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            data: –ù–∞–±–ª—é–¥–∞–≤–∞–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            errors: –ì—Ä–µ—à–∫–∏ –≤ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ç–∞
            
        Returns:
            œá¬≤ —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        residuals = (theory - data) / errors
        chi_squared = np.sum(residuals**2)
        return chi_squared
    
    @staticmethod
    def calculate_reduced_chi_squared(theory: np.ndarray, data: np.ndarray, 
                                   errors: np.ndarray, n_params: int) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ —Ä–µ–¥—É—Ü–∏—Ä–∞–Ω œá¬≤ (œá¬≤/dof)
        
        Args:
            theory: –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            data: –ù–∞–±–ª—é–¥–∞–≤–∞–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            errors: –ì—Ä–µ—à–∫–∏
            n_params: –ë—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ –º–æ–¥–µ–ª–∞
            
        Returns:
            –†–µ–¥—É—Ü–∏—Ä–∞–Ω œá¬≤
        """
        chi_squared = StatisticalAnalyzer.calculate_chi_squared(theory, data, errors)
        dof = len(data) - n_params
        return chi_squared / dof if dof > 0 else float('inf')
    
    @staticmethod
    def calculate_aic(chi_squared: float, n_params: int, n_data: int) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ Akaike Information Criterion
        
        Args:
            chi_squared: œá¬≤ —Å—Ç–æ–π–Ω–æ—Å—Ç
            n_params: –ë—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            n_data: –ë—Ä–æ–π —Ç–æ—á–∫–∏
            
        Returns:
            AIC —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        return chi_squared + 2 * n_params
    
    @staticmethod
    def calculate_bic(chi_squared: float, n_params: int, n_data: int) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ Bayesian Information Criterion
        
        Args:
            chi_squared: œá¬≤ —Å—Ç–æ–π–Ω–æ—Å—Ç
            n_params: –ë—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            n_data: –ë—Ä–æ–π —Ç–æ—á–∫–∏
            
        Returns:
            BIC —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        return chi_squared + n_params * np.log(n_data)
    
    @staticmethod
    def goodness_of_fit_summary(theory: np.ndarray, data: np.ndarray, 
                              errors: np.ndarray, n_params: int) -> Dict[str, float]:
        """
        –û–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ goodness of fit —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏—Ç–µ
        
        Args:
            theory: –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            data: –ù–∞–±–ª—é–¥–∞–≤–∞–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
            errors: –ì—Ä–µ—à–∫–∏
            n_params: –ë—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            
        Returns:
            –†–µ—á–Ω–∏–∫ —Å—ä—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        """
        n_data = len(data)
        chi_squared = StatisticalAnalyzer.calculate_chi_squared(theory, data, errors)
        
        return {
            'chi_squared': chi_squared,
            'reduced_chi_squared': StatisticalAnalyzer.calculate_reduced_chi_squared(
                theory, data, errors, n_params),
            'dof': n_data - n_params,
            'aic': StatisticalAnalyzer.calculate_aic(chi_squared, n_params, n_data),
            'bic': StatisticalAnalyzer.calculate_bic(chi_squared, n_params, n_data),
            'rms_residual': np.sqrt(np.mean(((theory - data) / errors)**2))
        }


def test_data_processing():
    """–¢–µ—Å—Ç –Ω–∞ –º–æ–¥—É–ª–∏—Ç–µ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –¥–∞–Ω–Ω–∏"""
    print("üß™ –¢–ï–°–¢ –ù–ê –û–ë–†–ê–ë–û–¢–ö–ê –ù–ê –î–ê–ù–ù–ò")
    print("=" * 50)
    
    # –¢–µ—Å—Ç –¥–∞–Ω–Ω–∏
    z_test = np.array([0.1, 0.3, 0.5, 0.7, 1.0])
    D_V_test = np.array([7.5, 8.2, 8.9, 9.5, 10.2])
    err_test = np.array([0.2, 0.15, 0.12, 0.18, 0.25])
    
    # BAO –ø—Ä–æ—Ü–µ—Å–æ—Ä
    print("\nüìä BAO –ü–†–û–¶–ï–°–û–†:")
    bao_processor = BAODataProcessor()
    processed_bao = bao_processor.process_bao_measurements(z_test, D_V_test, err_test)
    print(f"  –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ —Ç–æ—á–∫–∏: {processed_bao['statistics']['N_points']}")
    print(f"  –°—Ä–µ–¥–Ω–∞ –≥—Ä–µ—à–∫–∞: {processed_bao['statistics']['mean_error']:.3f}")
    
    # CMB –ø—Ä–æ—Ü–µ—Å–æ—Ä
    print("\nüå† CMB –ü–†–û–¶–ï–°–û–†:")
    l_test = np.arange(2, 101, 5)
    C_l_test = 5000 * np.exp(-l_test/500) + 100 * np.sin(l_test/50)
    C_l_err_test = 0.1 * C_l_test
    
    cmb_processor = CMBDataProcessor()
    processed_cmb = cmb_processor.process_cmb_power_spectrum(l_test, C_l_test, C_l_err_test)
    print(f"  –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ l —Ç–æ—á–∫–∏: {processed_cmb['statistics']['N_points']}")
    print(f"  –û—Ç–Ω–æ—Å–∏—Ç–µ–ª–Ω–∞ –≥—Ä–µ—à–∫–∞: {processed_cmb['statistics']['mean_relative_error']:.3f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑
    print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò –ê–ù–ê–õ–ò–ó:")
    theory_test = D_V_test + 0.1 * np.random.randn(len(D_V_test))
    stats = StatisticalAnalyzer.goodness_of_fit_summary(theory_test, D_V_test, err_test, 2)
    print(f"  œá¬≤: {stats['chi_squared']:.2f}")
    print(f"  –†–µ–¥—É—Ü–∏—Ä–∞–Ω œá¬≤: {stats['reduced_chi_squared']:.2f}")
    print(f"  AIC: {stats['aic']:.2f}")
    print(f"  BIC: {stats['bic']:.2f}")
    
    print("\n‚úÖ –í—Å–∏—á–∫–∏ —Ç–µ—Å—Ç–æ–≤–µ –∑–∞–≤—ä—Ä—à–∏—Ö–∞ —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    test_data_processing() 