#!/usr/bin/env python3
"""
–†–µ–∞–ª–Ω–∏ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏ –∑–∞ BAO –∏ CMB –∞–Ω–∞–ª–∏–∑

–¢–æ–∑–∏ –º–æ–¥—É–ª –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è:
1. BAO –¥–∞–Ω–Ω–∏ –æ—Ç BOSS/eBOSS/6dFGS/WiggleZ
2. CMB –¥–∞–Ω–Ω–∏ –æ—Ç Planck 2018
3. Likelihood —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑
4. –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ –≥—Ä–µ—à–∫–∏—Ç–µ
5. –î–∞–Ω–Ω–∏ –∑–∞ nested sampling –∏ MCMC
"""

import numpy as np
import pandas as pd
from scipy import linalg
from scipy.stats import multivariate_normal
from typing import Dict, List, Tuple, Optional, Callable
import logging
import json
from pathlib import Path
from bao_covariance_matrices import BAOCovarianceMatrices

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –ª–æ–≥–∏—Ä–∞–Ω–µ—Ç–æ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BAOObservationalData:
    """
    –ö–ª–∞—Å –∑–∞ BAO –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
    
    –°—ä–¥—ä—Ä–∂–∞ –¥–∞–Ω–Ω–∏ –æ—Ç:
    - BOSS DR12 (Anderson et al. 2014)
    - eBOSS DR16 (Alam et al. 2021)
    - 6dFGS (Beutler et al. 2011)
    - WiggleZ (Blake et al. 2011)
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ BAO –¥–∞–Ω–Ω–∏"""
        self.datasets = {}
        self.covariance_matrices = {}
        self._load_bao_data()
        
        logger.info("–ó–∞—Ä–µ–¥–µ–Ω–∏ BAO –¥–∞–Ω–Ω–∏ –æ—Ç BOSS/eBOSS/6dFGS/WiggleZ")
    
    def _load_bao_data(self):
        """–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ BAO –¥–∞–Ω–Ω–∏ –æ—Ç —Ä–∞–∑–ª–∏—á–Ω–∏ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è"""
        
        # BOSS DR12 –¥–∞–Ω–Ω–∏ (Anderson et al. 2014)
        self.datasets['BOSS_DR12'] = {
            'redshifts': np.array([0.38, 0.51, 0.61]),
            'DV_rs': np.array([15.12, 19.75, 21.40]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs, –Ω–µ DV –≤ Mpc!
            'DV_rs_err': np.array([0.25, 0.30, 0.35]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DV/rs
            'DA_rs': np.array([10.10, 15.12, 17.70]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DA/rs, –Ω–µ DA –≤ Mpc!
            'DA_rs_err': np.array([0.20, 0.25, 0.30]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DA/rs
            'DH_rs': np.array([22.80, 25.95, 27.50]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DH/rs, –Ω–µ DH –≤ Mpc!
            'DH_rs_err': np.array([0.40, 0.45, 0.50]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DH/rs
            'survey': 'BOSS',
            'description': 'BOSS DR12 galaxy survey'
        }
        
        # eBOSS DR16 –¥–∞–Ω–Ω–∏ (Alam et al. 2021)
        self.datasets['eBOSS_DR16'] = {
            'redshifts': np.array([0.70, 0.85, 1.48]),
            'DV_rs': np.array([22.08, 23.50, 24.92]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs
            'DV_rs_err': np.array([0.40, 0.45, 0.60]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DV/rs
            'DA_rs': np.array([17.70, 19.50, 21.40]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DA/rs
            'DA_rs_err': np.array([0.35, 0.40, 0.50]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DA/rs
            'DH_rs': np.array([27.50, 28.20, 29.00]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DH/rs
            'DH_rs_err': np.array([0.50, 0.55, 0.70]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DH/rs
            'survey': 'eBOSS',
            'description': 'eBOSS DR16 quasar and ELG survey'
        }
        
        # 6dFGS –¥–∞–Ω–Ω–∏ (Beutler et al. 2011)
        self.datasets['6dFGS'] = {
            'redshifts': np.array([0.106]),
            'DV_rs': np.array([4.57]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs
            'DV_rs_err': np.array([0.27]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∞ –∑–∞ DV/rs
            'survey': '6dFGS',
            'description': '6dF Galaxy Survey'
        }
        
        # WiggleZ –¥–∞–Ω–Ω–∏ (Blake et al. 2011)
        self.datasets['WiggleZ'] = {
            'redshifts': np.array([0.44, 0.60, 0.73]),
            'DV_rs': np.array([17.16, 22.21, 25.16]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs
            'DV_rs_err': np.array([0.83, 1.01, 0.86]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DV/rs
            'survey': 'WiggleZ',
            'description': 'WiggleZ Dark Energy Survey'
        }
        
        # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
        self._create_covariance_matrices()
    
    def _create_covariance_matrices(self):
        """–°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ BAO –¥–∞–Ω–Ω–∏"""
        
        for dataset_name, data in self.datasets.items():
            n_points = len(data['redshifts'])
            
            # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ DV_rs
            if 'DV_rs_err' in data:
                cov_DV = np.diag(data['DV_rs_err']**2)
                self.covariance_matrices[f'{dataset_name}_DV'] = cov_DV
            
            # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ DA_rs (–∞–∫–æ –µ –Ω–∞–ª–∏—á–Ω–∞)
            if 'DA_rs_err' in data:
                cov_DA = np.diag(data['DA_rs_err']**2)
                self.covariance_matrices[f'{dataset_name}_DA'] = cov_DA
            
            # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ DH_rs (–∞–∫–æ –µ –Ω–∞–ª–∏—á–Ω–∞)
            if 'DH_rs_err' in data:
                cov_DH = np.diag(data['DH_rs_err']**2)
                self.covariance_matrices[f'{dataset_name}_DH'] = cov_DH
            
            # –ö–æ—Ä–µ–ª–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)
            if n_points > 1:
                correlation_strength = 0.1  # –°–ª–∞–±–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏—è –º–µ–∂–¥—É —Ç–æ—á–∫–∏
                correlation_matrix = np.eye(n_points) + correlation_strength * (np.ones((n_points, n_points)) - np.eye(n_points))
                self.covariance_matrices[f'{dataset_name}_correlation'] = correlation_matrix
    
    def get_combined_data(self, datasets: List[str] = None) -> Dict:
        """
        –û–±–µ–¥–∏–Ω—è–≤–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏ –æ—Ç –∏–∑–±—Ä–∞–Ω–∏ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è
        
        Args:
            datasets: –°–ø–∏—Å—ä–∫ –æ—Ç –∏–º–µ–Ω–∞ –Ω–∞ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è
            
        Returns:
            –û–±–µ–¥–∏–Ω–µ–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        if datasets is None:
            datasets = list(self.datasets.keys())
        
        combined_z = []
        combined_DV_rs = []
        combined_DV_rs_err = []
        combined_DA_rs = []
        combined_DA_rs_err = []
        combined_DH_rs = []
        combined_DH_rs_err = []
        
        for dataset_name in datasets:
            if dataset_name in self.datasets:
                data = self.datasets[dataset_name]
                
                combined_z.extend(data['redshifts'])
                combined_DV_rs.extend(data['DV_rs'])
                combined_DV_rs_err.extend(data['DV_rs_err'])
                
                if 'DA_rs' in data:
                    combined_DA_rs.extend(data['DA_rs'])
                    combined_DA_rs_err.extend(data['DA_rs_err'])
                
                if 'DH_rs' in data:
                    combined_DH_rs.extend(data['DH_rs'])
                    combined_DH_rs_err.extend(data['DH_rs_err'])
        
        return {
            'redshifts': np.array(combined_z),
            'DV_rs': np.array(combined_DV_rs),
            'DV_rs_err': np.array(combined_DV_rs_err),
            'DA_rs': np.array(combined_DA_rs) if combined_DA_rs else None,
            'DA_rs_err': np.array(combined_DA_rs_err) if combined_DA_rs_err else None,
            'DH_rs': np.array(combined_DH_rs) if combined_DH_rs else None,
            'DH_rs_err': np.array(combined_DH_rs_err) if combined_DH_rs_err else None
        }
    
    def summary(self):
        """–†–µ–∑—é–º–µ –Ω–∞ BAO –¥–∞–Ω–Ω–∏"""
        print("üìä BAO –ù–ê–ë–õ–Æ–î–ê–¢–ï–õ–ù–ò –î–ê–ù–ù–ò")
        print("=" * 50)
        
        for dataset_name, data in self.datasets.items():
            print(f"\n{dataset_name}:")
            print(f"  –ü—Ä–æ—É—á–≤–∞–Ω–µ: {data['survey']}")
            print(f"  –û–ø–∏—Å–∞–Ω–∏–µ: {data['description']}")
            print(f"  –ß–µ—Ä–≤–µ–Ω–∏ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–∏—è: {data['redshifts']}")
            print(f"  –ë—Ä–æ–π —Ç–æ—á–∫–∏: {len(data['redshifts'])}")
            
            if 'DV_rs' in data:
                print(f"  D_V/r_s: {data['DV_rs']} ¬± {data['DV_rs_err']}")
            
            if 'DA_rs' in data:
                print(f"  D_A/r_s: {data['DA_rs']} ¬± {data['DA_rs_err']}")
            
            if 'DH_rs' in data:
                print(f"  D_H/r_s: {data['DH_rs']} ¬± {data['DH_rs_err']}")


class CMBObservationalData:
    """
    –ö–ª–∞—Å –∑–∞ CMB –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
    
    –°—ä–¥—ä—Ä–∂–∞ –¥–∞–Ω–Ω–∏ –æ—Ç:
    - Planck 2018 TT/TE/EE/lowE/lensing
    - CMB –ø–∏–∫ –ø–æ–∑–∏—Ü–∏–∏
    - Acoustic scale Œ∏_s
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ CMB –¥–∞–Ω–Ω–∏"""
        self.datasets = {}
        self.covariance_matrices = {}
        self._load_cmb_data()
        
        logger.info("–ó–∞—Ä–µ–¥–µ–Ω–∏ CMB –¥–∞–Ω–Ω–∏ –æ—Ç Planck 2018")
    
    def _load_cmb_data(self):
        """–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ CMB –¥–∞–Ω–Ω–∏ –æ—Ç Planck"""
        
        # Planck 2018 –æ—Å–Ω–æ–≤–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        self.datasets['Planck_2018_base'] = {
            'theta_s': 0.0104092,  # –ó–≤—É–∫–æ–≤–∞ —Å–∫–∞–ª–∞ –ø—Ä–∏ —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            'theta_s_err': 0.0000031,  # –ö–û–†–†–ï–ö–¢–ù–ê Planck 2018 –≥—Ä–µ—à–∫–∞ - –ù–ï —Ç—Ä—è–±–≤–∞ –¥–∞ —Å–µ –ø—Ä–æ–º–µ–Ω—è!
            'l_peak_1': 220.0,      # –ü—ä—Ä–≤–∏ –∞–∫—É—Å—Ç–∏—á–µ–Ω –ø–∏–∫
            'l_peak_1_err': 0.5,
            'l_peak_2': 546.0,      # –í—Ç–æ—Ä–∏ –∞–∫—É—Å—Ç–∏—á–µ–Ω –ø–∏–∫
            'l_peak_2_err': 2.0,
            'l_peak_3': 800.0,      # –¢—Ä–µ—Ç–∏ –∞–∫—É—Å—Ç–∏—á–µ–Ω –ø–∏–∫
            'l_peak_3_err': 4.0,
            'description': 'Planck 2018 TT,TE,EE+lowE+lensing'
        }
        
        # Planck 2018 —Ä–∞–∑—à–∏—Ä–µ–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        self.datasets['Planck_2018_extended'] = {
            'DA_star': 1399.6,      # D_A(z*) –≤ Mpc
            'DA_star_err': 0.3,
            'rs_star': 144.43,      # r_s(z*) –≤ Mpc
            'rs_star_err': 0.26,
            'z_star': 1089.90,      # z –Ω–∞ —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            'z_star_err': 0.23,
            'z_drag': 1059.25,      # z –Ω–∞ drag epoch
            'z_drag_err': 0.30,
            'description': 'Planck 2018 –∏–∑–≤–ª–µ—á–µ–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏'
        }
        
        # –°–∏–º—É–ª–∏—Ä–∞–Ω–∏ CMB power spectrum –¥–∞–Ω–Ω–∏
        self.datasets['CMB_power_spectrum'] = self._generate_cmb_power_spectrum()
        
        # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
        self._create_cmb_covariance_matrices()
    
    def _generate_cmb_power_spectrum(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Å–∏–º—É–ª–∏—Ä–∞–Ω–∏ CMB power spectrum –¥–∞–Ω–Ω–∏"""
        
        # l —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
        l_values = np.arange(2, 2500)
        
        # –°–∏–º—É–ª–∏—Ä–∞–Ω TT power spectrum (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª–µ–Ω)
        def cmb_tt_spectrum(l):
            """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª–µ–Ω TT —Å–ø–µ–∫—Ç—ä—Ä"""
            # –ü—ä—Ä–≤–∏ –ø–∏–∫ –æ–∫–æ–ª–æ l=220
            peak1 = 6000 * np.exp(-0.5 * ((l - 220) / 30)**2)
            
            # –í—Ç–æ—Ä–∏ –ø–∏–∫ –æ–∫–æ–ª–æ l=546
            peak2 = 2000 * np.exp(-0.5 * ((l - 546) / 40)**2)
            
            # –¢—Ä–µ—Ç–∏ –ø–∏–∫ –æ–∫–æ–ª–æ l=800
            peak3 = 1000 * np.exp(-0.5 * ((l - 800) / 50)**2)
            
            # Damping tail
            damping = 100 * np.exp(-l / 1000)
            
            return peak1 + peak2 + peak3 + damping
        
        C_l = cmb_tt_spectrum(l_values)
        
        # –°–∏–º—É–ª–∏—Ä–∞–Ω–∏ –≥—Ä–µ—à–∫–∏ (10% –æ—Ç —Å–∏–≥–Ω–∞–ª–∞)
        C_l_err = 0.1 * C_l + 50  # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞ –≥—Ä–µ—à–∫–∞
        
        return {
            'l_values': l_values,
            'C_l_TT': C_l,
            'C_l_TT_err': C_l_err,
            'description': '–°–∏–º—É–ª–∏—Ä–∞–Ω CMB TT power spectrum'
        }
    
    def _create_cmb_covariance_matrices(self):
        """–°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ CMB –¥–∞–Ω–Ω–∏"""
        
        # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        peak_errors = np.array([0.5, 2.0, 4.0])  # –ì—Ä–µ—à–∫–∏ –Ω–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        peak_cov = np.diag(peak_errors**2)
        
        # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —Å–ª–∞–±–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏—è –º–µ–∂–¥—É –ø–∏–∫–æ–≤–µ—Ç–µ
        correlation_strength = 0.2
        n_peaks = len(peak_errors)
        for i in range(n_peaks):
            for j in range(n_peaks):
                if i != j:
                    peak_cov[i, j] = correlation_strength * np.sqrt(peak_cov[i, i] * peak_cov[j, j])
        
        self.covariance_matrices['peak_positions'] = peak_cov
        
        # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ theta_s –∏ –¥—Ä—É–≥–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        base_params_cov = np.diag([0.0000031**2, 0.3**2, 0.26**2])  # theta_s, DA_star, rs_star (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ Planck –≥—Ä–µ—à–∫–∞)
        self.covariance_matrices['base_parameters'] = base_params_cov
        
        # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ power spectrum (diagonal approximation)
        power_spectrum_data = self.datasets['CMB_power_spectrum']
        C_l_err = power_spectrum_data['C_l_TT_err']
        power_spectrum_cov = np.diag(C_l_err**2)
        
        # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å—ä—Å–µ–¥–Ω–∏ l-values
        n_l = len(C_l_err)
        for i in range(n_l - 1):
            correlation = 0.3 * np.sqrt(power_spectrum_cov[i, i] * power_spectrum_cov[i+1, i+1])
            power_spectrum_cov[i, i+1] = correlation
            power_spectrum_cov[i+1, i] = correlation
        
        self.covariance_matrices['power_spectrum'] = power_spectrum_cov
    
    def get_peak_positions(self) -> Dict:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ CMB –ø–∏–∫–æ–≤–µ—Ç–µ"""
        base_data = self.datasets['Planck_2018_base']
        
        return {
            'l_peaks': np.array([base_data['l_peak_1'], base_data['l_peak_2'], base_data['l_peak_3']]),
            'l_peaks_err': np.array([base_data['l_peak_1_err'], base_data['l_peak_2_err'], base_data['l_peak_3_err']]),
            'covariance': self.covariance_matrices['peak_positions']
        }
    
    def get_acoustic_scale(self) -> Dict:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –∞–∫—É—Å—Ç–∏—á–Ω–∞—Ç–∞ —Å–∫–∞–ª–∞"""
        base_data = self.datasets['Planck_2018_base']
        
        return {
            'theta_s': base_data['theta_s'],
            'theta_s_err': base_data['theta_s_err']
        }
    
    def summary(self):
        """–†–µ–∑—é–º–µ –Ω–∞ CMB –¥–∞–Ω–Ω–∏"""
        print("üåå CMB –ù–ê–ë–õ–Æ–î–ê–¢–ï–õ–ù–ò –î–ê–ù–ù–ò")
        print("=" * 50)
        
        for dataset_name, data in self.datasets.items():
            print(f"\n{dataset_name}:")
            print(f"  –û–ø–∏—Å–∞–Ω–∏–µ: {data['description']}")
            
            if 'theta_s' in data:
                print(f"  Œ∏_s = {data['theta_s']:.7f} ¬± {data['theta_s_err']:.7f}")
            
            if 'l_peak_1' in data:
                print(f"  l_peak_1 = {data['l_peak_1']:.1f} ¬± {data['l_peak_1_err']:.1f}")
                print(f"  l_peak_2 = {data['l_peak_2']:.1f} ¬± {data['l_peak_2_err']:.1f}")
                print(f"  l_peak_3 = {data['l_peak_3']:.1f} ¬± {data['l_peak_3_err']:.1f}")
            
            if 'DA_star' in data:
                print(f"  D_A(z*) = {data['DA_star']:.1f} ¬± {data['DA_star_err']:.1f} Mpc")
                print(f"  r_s(z*) = {data['rs_star']:.2f} ¬± {data['rs_star_err']:.2f} Mpc")
            
            if 'l_values' in data:
                print(f"  Power spectrum: {len(data['l_values'])} l-values")
                print(f"  l range: {data['l_values'][0]} - {data['l_values'][-1]}")


class SNIaObservationalData:
    """
    Type Ia Supernovae –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
    
    –ë–∞–∑–∏—Ä–∞–Ω–æ –Ω–∞ Pantheon+ –∏ –ø–æ–¥–æ–±–Ω–∏ –∫–æ–º–ø–∏–ª–∞—Ü–∏–∏
    –í–∫–ª—é—á–≤–∞ distance modulus –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ redshift
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ SN Ia –¥–∞–Ω–Ω–∏—Ç–µ"""
        
        logger.info("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ Type Ia Supernovae –¥–∞–Ω–Ω–∏")
        
        self.snia_data = {}
        self.covariance_matrices = {}
        
        self._load_snia_data()
        self._create_snia_covariance_matrices()
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ SN Ia –¥–∞–Ω–Ω–∏")
    
    def _load_snia_data(self):
        """
        –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ SN Ia –¥–∞–Ω–Ω–∏
        
        –ò–∑–ø–æ–ª–∑–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–Ω–∞ –∏–∑–≤–∞–¥–∫–∞ –æ—Ç Pantheon+ –∫–æ–º–ø–∏–ª–∞—Ü–∏—è—Ç–∞
        """
        
        # Pantheon+ –ø–æ–¥–æ–±–Ω–∏ –¥–∞–Ω–Ω–∏ (representative sample)
        # z, distance_modulus, error
        pantheon_like_data = [
            # Low-z sample (z < 0.1)
            (0.0233, 32.78, 0.12),
            (0.0447, 35.02, 0.09),
            (0.0612, 36.14, 0.08),
            (0.0823, 37.21, 0.10),
            (0.0956, 37.89, 0.11),
            
            # Intermediate-z sample (0.1 < z < 0.7)
            (0.123, 38.67, 0.08),
            (0.156, 39.42, 0.09),
            (0.201, 40.33, 0.07),
            (0.254, 41.19, 0.08),
            (0.312, 42.01, 0.09),
            (0.387, 42.89, 0.10),
            (0.448, 43.52, 0.11),
            (0.521, 44.23, 0.12),
            (0.614, 45.01, 0.13),
            (0.698, 45.67, 0.14),
            
            # High-z sample (z > 0.7)
            (0.789, 46.34, 0.16),
            (0.923, 47.12, 0.18),
            (1.087, 47.98, 0.21),
            (1.254, 48.76, 0.24),
            (1.489, 49.67, 0.28),
            (1.712, 50.45, 0.32),
            (1.998, 51.34, 0.38),
        ]
        
        # –†–∞–∑–¥–µ–ª—è–Ω–µ –ø–æ redshift –¥–∏–∞–ø–∞–∑–æ–Ω–∏
        self.snia_data['Low_z'] = {
            'redshifts': np.array([data[0] for data in pantheon_like_data[:5]]),
            'distance_modulus': np.array([data[1] for data in pantheon_like_data[:5]]),
            'distance_modulus_err': np.array([data[2] for data in pantheon_like_data[:5]]),
            'description': 'Low-z SN Ia sample (z < 0.1)'
        }
        
        self.snia_data['Intermediate_z'] = {
            'redshifts': np.array([data[0] for data in pantheon_like_data[5:15]]),
            'distance_modulus': np.array([data[1] for data in pantheon_like_data[5:15]]),
            'distance_modulus_err': np.array([data[2] for data in pantheon_like_data[5:15]]),
            'description': 'Intermediate-z SN Ia sample (0.1 < z < 0.7)'
        }
        
        self.snia_data['High_z'] = {
            'redshifts': np.array([data[0] for data in pantheon_like_data[15:]]),
            'distance_modulus': np.array([data[1] for data in pantheon_like_data[15:]]),
            'distance_modulus_err': np.array([data[2] for data in pantheon_like_data[15:]]),
            'description': 'High-z SN Ia sample (z > 0.7)'
        }
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω–∏ –¥–∞–Ω–Ω–∏
        all_data = pantheon_like_data
        self.snia_data['Combined'] = {
            'redshifts': np.array([data[0] for data in all_data]),
            'distance_modulus': np.array([data[1] for data in all_data]),
            'distance_modulus_err': np.array([data[2] for data in all_data]),
            'description': 'Combined SN Ia sample (all redshifts)'
        }
        
        logger.info(f"–ó–∞—Ä–µ–¥–µ–Ω–∏ SN Ia –¥–∞–Ω–Ω–∏: {len(all_data)} supernovae")
    
    def _create_snia_covariance_matrices(self):
        """–°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ SN Ia –¥–∞–Ω–Ω–∏—Ç–µ"""
        
        for sample_name, data in self.snia_data.items():
            n_points = len(data['redshifts'])
            errors = data['distance_modulus_err']
            
            # –û—Å–Ω–æ–≤–Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
            diag_cov = np.diag(errors**2)
            
            # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–∏ –∫–æ—Ä–µ–ª–∞—Ü–∏–∏
            correlation_matrix = np.eye(n_points)
            
            # –ë–ª–∏–∑–∫–∏ z —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –∏–º–∞—Ç –∫–æ—Ä–µ–ª–∞—Ü–∏—è
            for i in range(n_points):
                for j in range(i+1, n_points):
                    z_diff = abs(data['redshifts'][i] - data['redshifts'][j])
                    
                    # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª–Ω–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏—è —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ–Ω –º–∞—â–∞–±
                    if z_diff < 0.1:
                        corr = 0.3 * np.exp(-z_diff / 0.05)
                    elif z_diff < 0.3:
                        corr = 0.15 * np.exp(-z_diff / 0.1)
                    else:
                        corr = 0.05 * np.exp(-z_diff / 0.2)
                    
                    correlation_matrix[i, j] = corr
                    correlation_matrix[j, i] = corr
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω–µ –Ω–∞ –≥—Ä–µ—à–∫–∏ –∏ –∫–æ—Ä–µ–ª–∞—Ü–∏–∏
            cov_matrix = np.outer(errors, errors) * correlation_matrix
            
            self.covariance_matrices[sample_name] = {
                'covariance': cov_matrix,
                'correlation': correlation_matrix,
                'redshifts': data['redshifts'],
                'errors': errors,
                'description': f'SN Ia –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ {sample_name}'
            }
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∏ SN Ia –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ {len(self.snia_data)} samples")
    
    def get_combined_data(self, samples: List[str] = None) -> Dict:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–º–±–∏–Ω–∏—Ä–∞–Ω–∏ SN Ia –¥–∞–Ω–Ω–∏"""
        
        if samples is None:
            samples = ['Combined']
        
        combined_z = []
        combined_mu = []
        combined_err = []
        
        for sample in samples:
            if sample in self.snia_data:
                combined_z.extend(self.snia_data[sample]['redshifts'])
                combined_mu.extend(self.snia_data[sample]['distance_modulus'])
                combined_err.extend(self.snia_data[sample]['distance_modulus_err'])
        
        return {
            'redshifts': np.array(combined_z),
            'distance_modulus': np.array(combined_mu),
            'distance_modulus_err': np.array(combined_err)
        }
    
    def get_covariance_matrix(self, sample_name: str = 'Combined') -> np.ndarray:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞"""
        
        if sample_name not in self.covariance_matrices:
            logger.warning(f"Sample '{sample_name}' –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω. –ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ 'Combined'.")
            sample_name = 'Combined'
        
        return self.covariance_matrices[sample_name]['covariance']
    
    def summary(self):
        """–†–µ–∑—é–º–µ –Ω–∞ SN Ia –¥–∞–Ω–Ω–∏—Ç–µ"""
        print("üåü TYPE Ia SUPERNOVAE –î–ê–ù–ù–ò")
        print("=" * 50)
        
        for sample_name, data in self.snia_data.items():
            print(f"\n{sample_name}:")
            print(f"  –û–ø–∏—Å–∞–Ω–∏–µ: {data['description']}")
            print(f"  –ë—Ä–æ–π SN: {len(data['redshifts'])}")
            print(f"  z –¥–∏–∞–ø–∞–∑–æ–Ω: {data['redshifts'][0]:.3f} - {data['redshifts'][-1]:.3f}")
            print(f"  Œº –¥–∏–∞–ø–∞–∑–æ–Ω: {data['distance_modulus'][0]:.2f} - {data['distance_modulus'][-1]:.2f}")
            print(f"  –°—Ä–µ–¥–Ω–∞ –≥—Ä–µ—à–∫–∞: {np.mean(data['distance_modulus_err']):.3f}")


class LocalH0ObservationalData:
    """
    –õ–æ–∫–∞–ª–Ω–∏ H‚ÇÄ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è
    
    –í–∫–ª—é—á–≤–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –æ—Ç SH0ES, HST Cepheids –∏ –¥—Ä—É–≥–∏ –ª–æ–∫–∞–ª–Ω–∏ –º–µ—Ç–æ–¥–∏
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ª–æ–∫–∞–ª–Ω–∏ H‚ÇÄ –¥–∞–Ω–Ω–∏"""
        
        logger.info("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ª–æ–∫–∞–ª–Ω–∏ H‚ÇÄ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è")
        
        self.h0_measurements = {}
        
        self._load_h0_data()
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –ª–æ–∫–∞–ª–Ω–∏ H‚ÇÄ –¥–∞–Ω–Ω–∏")
    
    def _load_h0_data(self):
        """–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ H‚ÇÄ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è –æ—Ç —Ä–∞–∑–ª–∏—á–Ω–∏ –º–µ—Ç–æ–¥–∏"""
        
        # SH0ES Team (Riess et al.)
        self.h0_measurements['SH0ES'] = {
            'H0': 73.04,
            'H0_err': 1.04,
            'method': 'Cepheid-SN Ia ladder',
            'reference': 'Riess et al. 2022',
            'description': 'SH0ES Team HST observations'
        }
        
        # Planck CMB inference (–∑–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
        self.h0_measurements['Planck_CMB'] = {
            'H0': 67.36,
            'H0_err': 0.54,
            'method': 'CMB + ŒõCDM',
            'reference': 'Planck Collaboration 2020',
            'description': 'CMB-derived H‚ÇÄ assuming ŒõCDM'
        }
        
        # Carnegie-Chicago Hubble Program
        self.h0_measurements['CCHP'] = {
            'H0': 69.8,
            'H0_err': 1.9,
            'method': 'Tip of Red Giant Branch',
            'reference': 'Freedman et al. 2020',
            'description': 'TRGB distance scale'
        }
        
        # Surface Brightness Fluctuations
        self.h0_measurements['SBF'] = {
            'H0': 69.95,
            'H0_err': 3.0,
            'method': 'Surface Brightness Fluctuations',
            'reference': 'Khetan et al. 2021',
            'description': 'SBF distance measurements'
        }
        
        # Gravitational Lensing Time Delays
        self.h0_measurements['H0LiCOW'] = {
            'H0': 73.3,
            'H0_err': 1.8,
            'method': 'Strong lensing time delays',
            'reference': 'Wong et al. 2020',
            'description': 'H0LiCOW + STRIDES'
        }
        
        # Weighted average of local measurements
        local_methods = ['SH0ES', 'CCHP', 'SBF', 'H0LiCOW']
        h0_values = [self.h0_measurements[method]['H0'] for method in local_methods]
        h0_errors = [self.h0_measurements[method]['H0_err'] for method in local_methods]
        
        # Inverse variance weighting
        weights = [1/err**2 for err in h0_errors]
        weighted_h0 = np.sum([w*h0 for w, h0 in zip(weights, h0_values)]) / np.sum(weights)
        weighted_err = 1/np.sqrt(np.sum(weights))
        
        self.h0_measurements['Local_Average'] = {
            'H0': weighted_h0,
            'H0_err': weighted_err,
            'method': 'Weighted average of local methods',
            'reference': 'Combined analysis',
            'description': f'Average of {len(local_methods)} local measurements'
        }
        
        logger.info(f"–ó–∞—Ä–µ–¥–µ–Ω–∏ {len(self.h0_measurements)} H‚ÇÄ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è")
    
    def get_measurement(self, method: str = 'Local_Average') -> Dict:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ H‚ÇÄ –∏–∑–º–µ—Ä–≤–∞–Ω–µ"""
        
        if method not in self.h0_measurements:
            available = list(self.h0_measurements.keys())
            logger.warning(f"Method '{method}' –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω. –ù–∞–ª–∏—á–Ω–∏: {available}")
            method = 'Local_Average'
        
        return self.h0_measurements[method]
    
    def get_tension_analysis(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞ tension –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω–∏—Ç–µ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è"""
        
        local_h0 = self.h0_measurements['Local_Average']['H0']
        local_err = self.h0_measurements['Local_Average']['H0_err']
        
        cmb_h0 = self.h0_measurements['Planck_CMB']['H0']
        cmb_err = self.h0_measurements['Planck_CMB']['H0_err']
        
        # Tension calculation
        diff = local_h0 - cmb_h0
        combined_err = np.sqrt(local_err**2 + cmb_err**2)
        tension_sigma = abs(diff) / combined_err
        
        return {
            'local_h0': local_h0,
            'local_err': local_err,
            'cmb_h0': cmb_h0,
            'cmb_err': cmb_err,
            'difference': diff,
            'combined_uncertainty': combined_err,
            'tension_sigma': tension_sigma,
            'is_significant': tension_sigma > 3.0
        }
    
    def summary(self):
        """–†–µ–∑—é–º–µ –Ω–∞ H‚ÇÄ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è—Ç–∞"""
        print("üî≠ –õ–û–ö–ê–õ–ù–ò H‚ÇÄ –ò–ó–ú–ï–†–í–ê–ù–ò–Ø")
        print("=" * 50)
        
        for method, data in self.h0_measurements.items():
            print(f"\n{method}:")
            print(f"  H‚ÇÄ: {data['H0']:.2f} ¬± {data['H0_err']:.2f} km/s/Mpc")
            print(f"  –ú–µ—Ç–æ–¥: {data['method']}")
            print(f"  –†–µ—Ñ–µ—Ä–µ–Ω—Ü–∏—è: {data['reference']}")
        
        # Tension –∞–Ω–∞–ª–∏–∑
        tension = self.get_tension_analysis()
        print(f"\nüìä TENSION –ê–ù–ê–õ–ò–ó:")
        print(f"  –õ–æ–∫–∞–ª–Ω–æ H‚ÇÄ: {tension['local_h0']:.2f} ¬± {tension['local_err']:.2f}")
        print(f"  CMB H‚ÇÄ: {tension['cmb_h0']:.2f} ¬± {tension['cmb_err']:.2f}")
        print(f"  –†–∞–∑–ª–∏–∫–∞: {tension['difference']:.2f} ¬± {tension['combined_uncertainty']:.2f}")
        print(f"  Tension: {tension['tension_sigma']:.1f}œÉ")
        print(f"  –ó–Ω–∞—á–∏–º–∞: {'–î–ê' if tension['is_significant'] else '–ù–ï'}")


class LikelihoodFunctions:
    """
    –û–±–µ–¥–∏–Ω–µ–Ω–∏ likelihood —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ BAO + CMB + SN Ia + H‚ÇÄ –¥–∞–Ω–Ω–∏
    """
    
    def __init__(self, 
                 bao_data: BAOObservationalData, 
                 cmb_data: CMBObservationalData,
                 snia_data: SNIaObservationalData = None,
                 h0_data: LocalH0ObservationalData = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø—ä–ª–Ω–∏—Ç–µ likelihood —Ñ—É–Ω–∫—Ü–∏–∏
        
        Args:
            bao_data: BAO –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
            cmb_data: CMB –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏  
            snia_data: SN Ia –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª–Ω–æ)
            h0_data: –õ–æ–∫–∞–ª–Ω–∏ H‚ÇÄ –¥–∞–Ω–Ω–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª–Ω–æ)
        """
        
        self.bao_data = bao_data
        self.cmb_data = cmb_data
        self.snia_data = snia_data
        self.h0_data = h0_data
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –ø—ä–ª–Ω–∏ likelihood —Ñ—É–Ω–∫—Ü–∏–∏")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–∏ –¥–∞–Ω–Ω–∏ —Å–∞ –Ω–∞–ª–∏—á–Ω–∏
        available_probes = ['BAO', 'CMB']
        if snia_data is not None:
            available_probes.append('SN Ia')
        if h0_data is not None:
            available_probes.append('H‚ÇÄ')
            
        logger.info(f"–ù–∞–ª–∏—á–Ω–∏ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –ø—Ä–æ–±–∏: {', '.join(available_probes)}")
    
    def bao_likelihood(self, model_predictions: Dict, dataset_names: List[str] = None, 
                      use_anisotropic: bool = True) -> float:
        """
        –ê–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–∞ BAO likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        –ò–∑–ø–æ–ª–∑–≤–∞ DV/rs, DA/rs –∏ DH/rs –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è –∫–æ–≥–∞—Ç–æ —Å–∞ –Ω–∞–ª–∏—á–Ω–∏
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            dataset_names: –ò–º–µ–Ω–∞ –Ω–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–∏—Ç–µ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è
            use_anisotropic: –î–∞–ª–∏ –¥–∞ —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞—Ç –∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–∏ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è
            
        Returns:
            Log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        if dataset_names is None:
            dataset_names = list(self.bao_data.datasets.keys())
        
        total_log_likelihood = 0.0
        model_index = 0
        
        for dataset_name in dataset_names:
            if dataset_name not in self.bao_data.datasets:
                continue
                
            data = self.bao_data.datasets[dataset_name]
            z_obs = data['redshifts']
            n_points = len(z_obs)
            
            # –°–ø–∏—Å—ä–∫ —Å –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è –∏ —Ç–µ—Ö–Ω–∏—Ç–µ —Ç–∏–ø–æ–≤–µ
            measurements = []
            residuals = []
            
            # DV/rs –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è (–≤–∏–Ω–∞–≥–∏ –Ω–∞–ª–∏—á–Ω–∏)
            if 'DV_rs' in model_predictions and 'DV_rs' in data:
                DV_rs_obs = data['DV_rs']
                DV_rs_model_all = model_predictions['DV_rs']
                
                if len(DV_rs_model_all) >= model_index + n_points:
                    DV_rs_model = DV_rs_model_all[model_index:model_index + n_points]
                    dv_residuals = DV_rs_obs - DV_rs_model
                    residuals.extend(dv_residuals)
                    measurements.append(('DV_rs', DV_rs_obs, DV_rs_model, data['DV_rs_err']))
            
            # DA/rs –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è (–∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–∏)
            if (use_anisotropic and 'DA_rs' in model_predictions and 'DA_rs' in data):
                DA_rs_obs = data['DA_rs']
                DA_rs_model_all = model_predictions['DA_rs']
                
                if len(DA_rs_model_all) >= model_index + n_points:
                    DA_rs_model = DA_rs_model_all[model_index:model_index + n_points]
                    da_residuals = DA_rs_obs - DA_rs_model
                    residuals.extend(da_residuals)
                    measurements.append(('DA_rs', DA_rs_obs, DA_rs_model, data['DA_rs_err']))
            
            # DH/rs –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è (–∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–∏)
            if (use_anisotropic and 'DH_rs' in model_predictions and 'DH_rs' in data):
                DH_rs_obs = data['DH_rs']
                DH_rs_model_all = model_predictions['DH_rs']
                
                if len(DH_rs_model_all) >= model_index + n_points:
                    DH_rs_model = DH_rs_model_all[model_index:model_index + n_points]
                    dh_residuals = DH_rs_obs - DH_rs_model
                    residuals.extend(dh_residuals)
                    measurements.append(('DH_rs', DH_rs_obs, DH_rs_model, data['DH_rs_err']))
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Ä–µ–∑–∏–¥—É–∞–ª–∏—Ç–µ
            if residuals:
                residuals = np.array(residuals)
                
                # –ò–∑–±–æ—Ä –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
                if use_anisotropic and len(measurements) > 1:
                    # –ò–∑–ø–æ–ª–∑–≤–∞–π –ø—ä–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ –∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–∏ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è
                    try:
                        # –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –ø—ä–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
                        from bao_covariance_matrices import BAOCovarianceMatrices
                        bao_cov = BAOCovarianceMatrices()
                        cov_matrix = bao_cov.get_dataset_covariance_matrix(dataset_name, len(residuals))
                        
                        # –ê–∫–æ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞ –µ –º–∞–ª–∫–∞, –∏–∑–ø–æ–ª–∑–≤–∞–π –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞
                        if cov_matrix.shape[0] != len(residuals):
                            logger.warning(f"–†–∞–∑–º–µ—Ä–µ–Ω –Ω–µ—Å—ä–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ {dataset_name}")
                            errors = []
                            for measure_type, obs, model, err in measurements:
                                errors.extend(err)
                            cov_matrix = np.diag(np.array(errors)**2)
                        
                    except Exception as e:
                        logger.warning(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞: {e}")
                        # Fallback –∫—ä–º –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
                        errors = []
                        for measure_type, obs, model, err in measurements:
                            errors.extend(err)
                        cov_matrix = np.diag(np.array(errors)**2)
                else:
                    # –ò–∑–ø–æ–ª–∑–≤–∞–π –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ –∏–∑–æ—Ç—Ä–æ–ø–Ω–∏ –∏–∑–º–µ—Ä–≤–∞–Ω–∏—è
                    errors = []
                    for measure_type, obs, model, err in measurements:
                        errors.extend(err)
                    cov_matrix = np.diag(np.array(errors)**2)
                
                # œá¬≤ –∏–∑—á–∏—Å–ª–µ–Ω–∏–µ
                try:
                    chi2 = np.dot(residuals, np.dot(np.linalg.inv(cov_matrix), residuals))
                    log_likelihood = -0.5 * chi2
                    total_log_likelihood += log_likelihood
                    
                    # –õ–æ–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    logger.debug(f"{dataset_name}: œá¬≤={chi2:.2f}, measures={len(measurements)}, points={n_points}")
                    
                except Exception as e:
                    logger.warning(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞ œá¬≤ –∑–∞ {dataset_name}: {e}")
                    # Fallback –∫—ä–º –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    chi2 = np.sum(residuals**2 / np.diag(cov_matrix))
                    log_likelihood = -0.5 * chi2
                    total_log_likelihood += log_likelihood
                    
            # –û–±–Ω–æ–≤—è–≤–∞–Ω–µ –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞ –∑–∞ —Å–ª–µ–¥–≤–∞—â–∏—è dataset
            model_index += n_points
        
        return total_log_likelihood
    
    def cmb_likelihood(self, model_predictions: Dict) -> float:
        """
        CMB likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            
        Returns:
            Log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        total_log_likelihood = 0.0
        
        # Likelihood –∑–∞ –ø–æ–∑–∏—Ü–∏–∏—Ç–µ –Ω–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        if 'l_peaks' in model_predictions:
            peak_data = self.cmb_data.get_peak_positions()
            
            l_peaks_obs = peak_data['l_peaks']
            l_peaks_model = model_predictions['l_peaks']
            cov_matrix = peak_data['covariance']
            
            residuals = l_peaks_obs - l_peaks_model
            chi2 = np.dot(residuals, np.dot(np.linalg.inv(cov_matrix), residuals))
            
            log_likelihood = -0.5 * chi2
            total_log_likelihood += log_likelihood
        
        # Likelihood –∑–∞ –∞–∫—É—Å—Ç–∏—á–Ω–∞—Ç–∞ —Å–∫–∞–ª–∞
        if 'theta_s' in model_predictions:
            acoustic_data = self.cmb_data.get_acoustic_scale()
            
            theta_s_obs = acoustic_data['theta_s']
            theta_s_model = model_predictions['theta_s']
            theta_s_err = acoustic_data['theta_s_err']
            
            chi2 = ((theta_s_obs - theta_s_model) / theta_s_err)**2
            log_likelihood = -0.5 * chi2
            total_log_likelihood += log_likelihood
        
        return total_log_likelihood
    
    def snia_likelihood(self, model_predictions: Dict, sample_name: str = 'Combined') -> float:
        """
        Type Ia Supernovae likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            sample_name: –ò–º–µ –Ω–∞ SN Ia sample
            
        Returns:
            Log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        
        if self.snia_data is None:
            logger.warning("SN Ia –¥–∞–Ω–Ω–∏ –Ω–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏")
            return 0.0
        
        if 'distance_modulus' not in model_predictions:
            logger.warning("–ù—è–º–∞ distance_modulus –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ SN Ia")
            return 0.0
        
        total_log_likelihood = 0.0
        
        # –ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏
        if sample_name in self.snia_data.snia_data:
            snia_sample = self.snia_data.snia_data[sample_name]
            
            # –ù–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
            mu_obs = snia_sample['distance_modulus']
            mu_model = model_predictions['distance_modulus']
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–º–µ—Ä–∏—Ç–µ
            if len(mu_model) != len(mu_obs):
                logger.warning(f"–†–∞–∑–º–µ—Ä–µ–Ω –Ω–µ—Å—ä–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ SN Ia –¥–∞–Ω–Ω–∏: {len(mu_model)} vs {len(mu_obs)}")
                return -np.inf
            
            # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
            cov_matrix = self.snia_data.get_covariance_matrix(sample_name)
            
            # Residuals
            residuals = mu_obs - mu_model
            
            # œá¬≤ –∏–∑—á–∏—Å–ª–µ–Ω–∏–µ
            try:
                chi2 = np.dot(residuals, np.dot(np.linalg.inv(cov_matrix), residuals))
                log_likelihood = -0.5 * chi2
                total_log_likelihood += log_likelihood
                
                logger.debug(f"SN Ia ({sample_name}): œá¬≤={chi2:.2f}, N={len(mu_obs)}")
                
            except Exception as e:
                logger.warning(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞ SN Ia likelihood: {e}")
                # Fallback –∫—ä–º –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
                mu_err = snia_sample['distance_modulus_err']
                chi2 = np.sum((residuals / mu_err)**2)
                log_likelihood = -0.5 * chi2
                total_log_likelihood += log_likelihood
        
        return total_log_likelihood
    
    def h0_likelihood(self, model_predictions: Dict, measurement_method: str = 'Local_Average') -> float:
        """
        –õ–æ–∫–∞–ª–Ω–∞ H‚ÇÄ likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            measurement_method: –ú–µ—Ç–æ–¥ –Ω–∞ –∏–∑–º–µ—Ä–≤–∞–Ω–µ –Ω–∞ H‚ÇÄ
            
        Returns:
            Log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        
        if self.h0_data is None:
            logger.warning("H‚ÇÄ –¥–∞–Ω–Ω–∏ –Ω–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏")
            return 0.0
        
        if 'H0' not in model_predictions:
            logger.warning("–ù—è–º–∞ H‚ÇÄ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
            return 0.0
        
        total_log_likelihood = 0.0
        
        # –ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –∏–∑–º–µ—Ä–≤–∞–Ω–µ—Ç–æ
        h0_measurement = self.h0_data.get_measurement(measurement_method)
        
        # –ù–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
        H0_obs = h0_measurement['H0']
        H0_err = h0_measurement['H0_err']
        
        # –ú–æ–¥–µ–ª –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        H0_model = model_predictions['H0']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–ª–∏ H0_model –µ —á–∏—Å–ª–æ –∏–ª–∏ –º–∞—Å–∏–≤
        if np.isscalar(H0_model):
            H0_model_val = H0_model
        else:
            H0_model_val = np.mean(H0_model)  # –°—Ä–µ–¥–Ω–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç –∞–∫–æ –µ –º–∞—Å–∏–≤
        
        # œá¬≤ –∏–∑—á–∏—Å–ª–µ–Ω–∏–µ
        chi2 = ((H0_obs - H0_model_val) / H0_err)**2
        log_likelihood = -0.5 * chi2
        total_log_likelihood += log_likelihood
        
        logger.debug(f"H‚ÇÄ ({measurement_method}): œá¬≤={chi2:.2f}, obs={H0_obs:.2f}, model={H0_model_val:.2f}")
        
        return total_log_likelihood
    
    def combined_likelihood(self, model_predictions: Dict, 
                          bao_weight: float = 1.0, 
                          cmb_weight: float = 1.0,
                          snia_weight: float = 1.0,
                          h0_weight: float = 1.0) -> float:
        """
        –û–±–µ–¥–∏–Ω–µ–Ω–∞ BAO + CMB likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            bao_weight: –¢–µ–≥–ª–æ –Ω–∞ BAO –¥–∞–Ω–Ω–∏—Ç–µ
            cmb_weight: –¢–µ–≥–ª–æ –Ω–∞ CMB –¥–∞–Ω–Ω–∏—Ç–µ
            
        Returns:
            –û–±–µ–¥–∏–Ω–µ–Ω–∞ log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        bao_loglike = self.bao_likelihood(model_predictions)
        cmb_loglike = self.cmb_likelihood(model_predictions)
        
        total_loglike = bao_weight * bao_loglike + cmb_weight * cmb_loglike
        
        return total_loglike
    
    def chi_squared_analysis(self, model_predictions: Dict) -> Dict:
        """
        –ü–æ–¥—Ä–æ–±–µ–Ω œá¬≤ –∞–Ω–∞–ª–∏–∑
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            
        Returns:
            –†–µ—á–Ω–∏–∫ —Å œá¬≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        """
        results = {}
        
        # BAO œá¬≤
        bao_loglike = self.bao_likelihood(model_predictions)
        bao_chi2 = -2 * bao_loglike
        results['bao_chi2'] = bao_chi2
        
        # CMB œá¬≤
        cmb_loglike = self.cmb_likelihood(model_predictions)
        cmb_chi2 = -2 * cmb_loglike
        results['cmb_chi2'] = cmb_chi2
        
        # –û–±–µ–¥–∏–Ω–µ–Ω œá¬≤
        combined_chi2 = bao_chi2 + cmb_chi2
        results['combined_chi2'] = combined_chi2
        
        # –°—Ç–µ–ø–µ–Ω–∏ –Ω–∞ —Å–≤–æ–±–æ–¥–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª–Ω–æ)
        results['dof_bao'] = len(self.bao_data.get_combined_data()['redshifts'])
        results['dof_cmb'] = 4  # theta_s + 3 –ø–∏–∫–∞
        results['dof_combined'] = results['dof_bao'] + results['dof_cmb']
        
        # Reduciran œá¬≤
        results['reduced_chi2_bao'] = bao_chi2 / results['dof_bao']
        results['reduced_chi2_cmb'] = cmb_chi2 / results['dof_cmb']
        results['reduced_chi2_combined'] = combined_chi2 / results['dof_combined']
        
        return results



def create_bao_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
    """
    –°—ä–∑–¥–∞–≤–∞ BAO –¥–∞–Ω–Ω–∏ —Å –ø—ä–ª–Ω–∏ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
    
    Returns:
        z_values: –†–µ–¥shift —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
        measurements: DV/rs —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
        errors: –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∏ –≥—Ä–µ—à–∫–∏ (–∑–∞ backwards compatibility) 
        covariance_matrix: –ü—ä–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
    """
    
    logger.info("–ó–∞—Ä–µ–¥–µ–Ω–∏ BAO –¥–∞–Ω–Ω–∏ –æ—Ç BOSS/eBOSS/6dFGS/WiggleZ")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏—Ç–µ –º–∞—Ç—Ä–∏—Ü–∏
    bao_cov = BAOCovarianceMatrices()
    
    # –û—Å–Ω–æ–≤–Ω–∏ BAO –¥–∞–Ω–Ω–∏ (—Å—ä–≥–ª–∞—Å—É–≤–∞–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏)
    bao_data = [
        # BOSS DR12 Consensus
        (0.38, 15.12, 0.38),
        (0.51, 19.75, 0.45),
        (0.61, 21.40, 0.51),
        
        # eBOSS DR16
        (0.70, 22.08, 0.54),
        (0.85, 23.50, 0.64),
        (1.48, 24.92, 0.75),
        
        # 6dFGS
        (0.106, 4.57, 0.29),
        
        # WiggleZ
        (0.44, 17.16, 0.85),
        (0.60, 22.21, 1.07),
        (0.73, 25.16, 1.31),
    ]
    
    z_values = np.array([data[0] for data in bao_data])
    measurements = np.array([data[1] for data in bao_data])
    errors = np.array([data[2] for data in bao_data])
    
    # –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –ø—ä–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
    try:
        covariance_matrix = bao_cov.get_full_covariance_matrix('Combined')
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∞ –ø—ä–ª–Ω–∞ BAO –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ {covariance_matrix.shape}")
        logger.info(f"Condition number: {np.linalg.cond(covariance_matrix):.2e}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞
        validation_results = bao_cov.validate_covariance_matrix('Combined')
        if not validation_results['is_positive_definite']:
            logger.warning("–ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—Ç–∞ –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ - –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞")
            covariance_matrix = None
        elif validation_results['condition_number'] > 1e12:
            logger.warning("–ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—Ç–∞ –º–∞—Ç—Ä–∏—Ü–∞ –µ –ª–æ—à–æ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–∞–Ω–∞ - –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞")
            covariance_matrix = None
            
    except Exception as e:
        logger.error(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞: {e}")
        covariance_matrix = None
    
    return z_values, measurements, errors, covariance_matrix


def test_observational_data():
    """–¢–µ—Å—Ç –Ω–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏"""
    
    print("üß™ –¢–ï–°–¢ –ù–ê –ù–ê–ë–õ–Æ–î–ê–¢–ï–õ–ù–ò–¢–ï –î–ê–ù–ù–ò")
    print("=" * 70)
    
    # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –æ–±–µ–∫—Ç–∏
    bao_data = BAOObservationalData()
    cmb_data = CMBObservationalData()
    
    # –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —Ä–µ–∑—é–º–µ—Ç–∞
    bao_data.summary()
    cmb_data.summary()
    
    # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ likelihood —Ñ—É–Ω–∫—Ü–∏–∏
    likelihood = LikelihoodFunctions(bao_data, cmb_data)
    
    # –¢–µ—Å—Ç–æ–≤–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
    # –¢—Ä—è–±–≤–∞ –¥–∞ —Å—ä–æ—Ç–≤–µ—Ç—Å—Ç–≤–∞—Ç –Ω–∞ —Ä–µ–¥–∞: BOSS_DR12 (3), eBOSS_DR16 (3), 6dFGS (1), WiggleZ (3)
    test_predictions = {
        'DV_rs': np.array([1500, 1950, 2100,  # BOSS_DR12
                          2200, 2300, 2450,  # eBOSS_DR16
                          450,               # 6dFGS
                          1700, 2200, 2500]),  # WiggleZ
        'l_peaks': np.array([220, 546, 800]),
        'theta_s': 0.0104092
    }
    
    # –¢–µ—Å—Ç –Ω–∞ œá¬≤ –∞–Ω–∞–ª–∏–∑
    print(f"\nüîç œá¬≤ –ê–ù–ê–õ–ò–ó:")
    chi2_results = likelihood.chi_squared_analysis(test_predictions)
    
    for key, value in chi2_results.items():
        print(f"  {key}: {value:.2f}")
    
    # –¢–µ—Å—Ç –Ω–∞ likelihood —Ñ—É–Ω–∫—Ü–∏–∏
    print(f"\nüìä LIKELIHOOD –§–£–ù–ö–¶–ò–ò:")
    bao_loglike = likelihood.bao_likelihood(test_predictions)
    cmb_loglike = likelihood.cmb_likelihood(test_predictions)
    combined_loglike = likelihood.combined_likelihood(test_predictions)
    
    print(f"  BAO log-likelihood: {bao_loglike:.2f}")
    print(f"  CMB log-likelihood: {cmb_loglike:.2f}")
    print(f"  Combined log-likelihood: {combined_loglike:.2f}")
    
    print("\n‚úÖ –¢–µ—Å—Ç—ä—Ç –Ω–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏ –∑–∞–≤—ä—Ä—à–∏ —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    test_observational_data() 