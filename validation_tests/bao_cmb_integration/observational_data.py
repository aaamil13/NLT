#!/usr/bin/env python3
"""
–†–µ–∞–ª–Ω–∏ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏ –∑–∞ BAO –∏ CMB –∞–Ω–∞–ª–∏–∑

–¢–æ–∑–∏ –º–æ–¥—É–ª –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è:
1. BAO –¥–∞–Ω–Ω–∏ –æ—Ç BOSS/eBOSS/6dFGS/WiggleZ
2. CMB –¥–∞–Ω–Ω–∏ –æ—Ç Planck 2018
3. Likelihood —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑
4. –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ –≥—Ä–µ—à–∫–∏—Ç–µ
5. –î–∞–Ω–Ω–∏ –∑–∞ nested sampling –∏ MCMC
"""

import numpy as np
import pandas as pd
from scipy import linalg
from scipy.stats import multivariate_normal
from typing import Dict, List, Tuple, Optional, Callable
import logging
import json
from pathlib import Path
from bao_covariance_matrices import BAOCovarianceMatrices

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –ª–æ–≥–∏—Ä–∞–Ω–µ—Ç–æ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BAOObservationalData:
    """
    –ö–ª–∞—Å –∑–∞ BAO –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
    
    –°—ä–¥—ä—Ä–∂–∞ –¥–∞–Ω–Ω–∏ –æ—Ç:
    - BOSS DR12 (Anderson et al. 2014)
    - eBOSS DR16 (Alam et al. 2021)
    - 6dFGS (Beutler et al. 2011)
    - WiggleZ (Blake et al. 2011)
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ BAO –¥–∞–Ω–Ω–∏"""
        self.datasets = {}
        self.covariance_matrices = {}
        self._load_bao_data()
        
        logger.info("–ó–∞—Ä–µ–¥–µ–Ω–∏ BAO –¥–∞–Ω–Ω–∏ –æ—Ç BOSS/eBOSS/6dFGS/WiggleZ")
    
    def _load_bao_data(self):
        """–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ BAO –¥–∞–Ω–Ω–∏ –æ—Ç —Ä–∞–∑–ª–∏—á–Ω–∏ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è"""
        
        # BOSS DR12 –¥–∞–Ω–Ω–∏ (Anderson et al. 2014)
        self.datasets['BOSS_DR12'] = {
            'redshifts': np.array([0.38, 0.51, 0.61]),
            'DV_rs': np.array([15.12, 19.75, 21.40]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs, –Ω–µ DV –≤ Mpc!
            'DV_rs_err': np.array([0.25, 0.30, 0.35]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DV/rs
            'DA_rs': np.array([10.10, 15.12, 17.70]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DA/rs, –Ω–µ DA –≤ Mpc!
            'DA_rs_err': np.array([0.20, 0.25, 0.30]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DA/rs
            'DH_rs': np.array([22.80, 25.95, 27.50]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DH/rs, –Ω–µ DH –≤ Mpc!
            'DH_rs_err': np.array([0.40, 0.45, 0.50]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DH/rs
            'survey': 'BOSS',
            'description': 'BOSS DR12 galaxy survey'
        }
        
        # eBOSS DR16 –¥–∞–Ω–Ω–∏ (Alam et al. 2021)
        self.datasets['eBOSS_DR16'] = {
            'redshifts': np.array([0.70, 0.85, 1.48]),
            'DV_rs': np.array([22.08, 23.50, 24.92]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs
            'DV_rs_err': np.array([0.40, 0.45, 0.60]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DV/rs
            'DA_rs': np.array([17.70, 19.50, 21.40]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DA/rs
            'DA_rs_err': np.array([0.35, 0.40, 0.50]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DA/rs
            'DH_rs': np.array([27.50, 28.20, 29.00]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DH/rs
            'DH_rs_err': np.array([0.50, 0.55, 0.70]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DH/rs
            'survey': 'eBOSS',
            'description': 'eBOSS DR16 quasar and ELG survey'
        }
        
        # 6dFGS –¥–∞–Ω–Ω–∏ (Beutler et al. 2011)
        self.datasets['6dFGS'] = {
            'redshifts': np.array([0.106]),
            'DV_rs': np.array([4.57]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs
            'DV_rs_err': np.array([0.27]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∞ –∑–∞ DV/rs
            'survey': '6dFGS',
            'description': '6dF Galaxy Survey'
        }
        
        # WiggleZ –¥–∞–Ω–Ω–∏ (Blake et al. 2011)
        self.datasets['WiggleZ'] = {
            'redshifts': np.array([0.44, 0.60, 0.73]),
            'DV_rs': np.array([17.16, 22.21, 25.16]),  # üö® –ü–û–ü–†–ê–í–ö–ê: DV/rs
            'DV_rs_err': np.array([0.83, 1.01, 0.86]),  # üö® –ü–û–ü–†–ê–í–ö–ê: –≥—Ä–µ—à–∫–∏ –∑–∞ DV/rs
            'survey': 'WiggleZ',
            'description': 'WiggleZ Dark Energy Survey'
        }
        
        # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
        self._create_covariance_matrices()
    
    def _create_covariance_matrices(self):
        """–°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ BAO –¥–∞–Ω–Ω–∏"""
        
        for dataset_name, data in self.datasets.items():
            n_points = len(data['redshifts'])
            
            # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ DV_rs
            if 'DV_rs_err' in data:
                cov_DV = np.diag(data['DV_rs_err']**2)
                self.covariance_matrices[f'{dataset_name}_DV'] = cov_DV
            
            # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ DA_rs (–∞–∫–æ –µ –Ω–∞–ª–∏—á–Ω–∞)
            if 'DA_rs_err' in data:
                cov_DA = np.diag(data['DA_rs_err']**2)
                self.covariance_matrices[f'{dataset_name}_DA'] = cov_DA
            
            # –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ DH_rs (–∞–∫–æ –µ –Ω–∞–ª–∏—á–Ω–∞)
            if 'DH_rs_err' in data:
                cov_DH = np.diag(data['DH_rs_err']**2)
                self.covariance_matrices[f'{dataset_name}_DH'] = cov_DH
            
            # –ö–æ—Ä–µ–ª–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)
            if n_points > 1:
                correlation_strength = 0.1  # –°–ª–∞–±–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏—è –º–µ–∂–¥—É —Ç–æ—á–∫–∏
                correlation_matrix = np.eye(n_points) + correlation_strength * (np.ones((n_points, n_points)) - np.eye(n_points))
                self.covariance_matrices[f'{dataset_name}_correlation'] = correlation_matrix
    
    def get_combined_data(self, datasets: List[str] = None) -> Dict:
        """
        –û–±–µ–¥–∏–Ω—è–≤–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏ –æ—Ç –∏–∑–±—Ä–∞–Ω–∏ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è
        
        Args:
            datasets: –°–ø–∏—Å—ä–∫ –æ—Ç –∏–º–µ–Ω–∞ –Ω–∞ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è
            
        Returns:
            –û–±–µ–¥–∏–Ω–µ–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        if datasets is None:
            datasets = list(self.datasets.keys())
        
        combined_z = []
        combined_DV_rs = []
        combined_DV_rs_err = []
        combined_DA_rs = []
        combined_DA_rs_err = []
        combined_DH_rs = []
        combined_DH_rs_err = []
        
        for dataset_name in datasets:
            if dataset_name in self.datasets:
                data = self.datasets[dataset_name]
                
                combined_z.extend(data['redshifts'])
                combined_DV_rs.extend(data['DV_rs'])
                combined_DV_rs_err.extend(data['DV_rs_err'])
                
                if 'DA_rs' in data:
                    combined_DA_rs.extend(data['DA_rs'])
                    combined_DA_rs_err.extend(data['DA_rs_err'])
                
                if 'DH_rs' in data:
                    combined_DH_rs.extend(data['DH_rs'])
                    combined_DH_rs_err.extend(data['DH_rs_err'])
        
        return {
            'redshifts': np.array(combined_z),
            'DV_rs': np.array(combined_DV_rs),
            'DV_rs_err': np.array(combined_DV_rs_err),
            'DA_rs': np.array(combined_DA_rs) if combined_DA_rs else None,
            'DA_rs_err': np.array(combined_DA_rs_err) if combined_DA_rs_err else None,
            'DH_rs': np.array(combined_DH_rs) if combined_DH_rs else None,
            'DH_rs_err': np.array(combined_DH_rs_err) if combined_DH_rs_err else None
        }
    
    def summary(self):
        """–†–µ–∑—é–º–µ –Ω–∞ BAO –¥–∞–Ω–Ω–∏"""
        print("üìä BAO –ù–ê–ë–õ–Æ–î–ê–¢–ï–õ–ù–ò –î–ê–ù–ù–ò")
        print("=" * 50)
        
        for dataset_name, data in self.datasets.items():
            print(f"\n{dataset_name}:")
            print(f"  –ü—Ä–æ—É—á–≤–∞–Ω–µ: {data['survey']}")
            print(f"  –û–ø–∏—Å–∞–Ω–∏–µ: {data['description']}")
            print(f"  –ß–µ—Ä–≤–µ–Ω–∏ –æ—Ç–º–µ—Å—Ç–≤–∞–Ω–∏—è: {data['redshifts']}")
            print(f"  –ë—Ä–æ–π —Ç–æ—á–∫–∏: {len(data['redshifts'])}")
            
            if 'DV_rs' in data:
                print(f"  D_V/r_s: {data['DV_rs']} ¬± {data['DV_rs_err']}")
            
            if 'DA_rs' in data:
                print(f"  D_A/r_s: {data['DA_rs']} ¬± {data['DA_rs_err']}")
            
            if 'DH_rs' in data:
                print(f"  D_H/r_s: {data['DH_rs']} ¬± {data['DH_rs_err']}")


class CMBObservationalData:
    """
    –ö–ª–∞—Å –∑–∞ CMB –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
    
    –°—ä–¥—ä—Ä–∂–∞ –¥–∞–Ω–Ω–∏ –æ—Ç:
    - Planck 2018 TT/TE/EE/lowE/lensing
    - CMB –ø–∏–∫ –ø–æ–∑–∏—Ü–∏–∏
    - Acoustic scale Œ∏_s
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ CMB –¥–∞–Ω–Ω–∏"""
        self.datasets = {}
        self.covariance_matrices = {}
        self._load_cmb_data()
        
        logger.info("–ó–∞—Ä–µ–¥–µ–Ω–∏ CMB –¥–∞–Ω–Ω–∏ –æ—Ç Planck 2018")
    
    def _load_cmb_data(self):
        """–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ CMB –¥–∞–Ω–Ω–∏ –æ—Ç Planck"""
        
        # Planck 2018 –æ—Å–Ω–æ–≤–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        self.datasets['Planck_2018_base'] = {
            'theta_s': 0.0104092,  # –ó–≤—É–∫–æ–≤–∞ —Å–∫–∞–ª–∞ –ø—Ä–∏ —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            'theta_s_err': 0.0000031,  # –ö–û–†–†–ï–ö–¢–ù–ê Planck 2018 –≥—Ä–µ—à–∫–∞ - –ù–ï —Ç—Ä—è–±–≤–∞ –¥–∞ —Å–µ –ø—Ä–æ–º–µ–Ω—è!
            'l_peak_1': 220.0,      # –ü—ä—Ä–≤–∏ –∞–∫—É—Å—Ç–∏—á–µ–Ω –ø–∏–∫
            'l_peak_1_err': 0.5,
            'l_peak_2': 546.0,      # –í—Ç–æ—Ä–∏ –∞–∫—É—Å—Ç–∏—á–µ–Ω –ø–∏–∫
            'l_peak_2_err': 2.0,
            'l_peak_3': 800.0,      # –¢—Ä–µ—Ç–∏ –∞–∫—É—Å—Ç–∏—á–µ–Ω –ø–∏–∫
            'l_peak_3_err': 4.0,
            'description': 'Planck 2018 TT,TE,EE+lowE+lensing'
        }
        
        # Planck 2018 —Ä–∞–∑—à–∏—Ä–µ–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        self.datasets['Planck_2018_extended'] = {
            'DA_star': 1399.6,      # D_A(z*) –≤ Mpc
            'DA_star_err': 0.3,
            'rs_star': 144.43,      # r_s(z*) –≤ Mpc
            'rs_star_err': 0.26,
            'z_star': 1089.90,      # z –Ω–∞ —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            'z_star_err': 0.23,
            'z_drag': 1059.25,      # z –Ω–∞ drag epoch
            'z_drag_err': 0.30,
            'description': 'Planck 2018 –∏–∑–≤–ª–µ—á–µ–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏'
        }
        
        # –°–∏–º—É–ª–∏—Ä–∞–Ω–∏ CMB power spectrum –¥–∞–Ω–Ω–∏
        self.datasets['CMB_power_spectrum'] = self._generate_cmb_power_spectrum()
        
        # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
        self._create_cmb_covariance_matrices()
    
    def _generate_cmb_power_spectrum(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Å–∏–º—É–ª–∏—Ä–∞–Ω–∏ CMB power spectrum –¥–∞–Ω–Ω–∏"""
        
        # l —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
        l_values = np.arange(2, 2500)
        
        # –°–∏–º—É–ª–∏—Ä–∞–Ω TT power spectrum (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª–µ–Ω)
        def cmb_tt_spectrum(l):
            """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª–µ–Ω TT —Å–ø–µ–∫—Ç—ä—Ä"""
            # –ü—ä—Ä–≤–∏ –ø–∏–∫ –æ–∫–æ–ª–æ l=220
            peak1 = 6000 * np.exp(-0.5 * ((l - 220) / 30)**2)
            
            # –í—Ç–æ—Ä–∏ –ø–∏–∫ –æ–∫–æ–ª–æ l=546
            peak2 = 2000 * np.exp(-0.5 * ((l - 546) / 40)**2)
            
            # –¢—Ä–µ—Ç–∏ –ø–∏–∫ –æ–∫–æ–ª–æ l=800
            peak3 = 1000 * np.exp(-0.5 * ((l - 800) / 50)**2)
            
            # Damping tail
            damping = 100 * np.exp(-l / 1000)
            
            return peak1 + peak2 + peak3 + damping
        
        C_l = cmb_tt_spectrum(l_values)
        
        # –°–∏–º—É–ª–∏—Ä–∞–Ω–∏ –≥—Ä–µ—à–∫–∏ (10% –æ—Ç —Å–∏–≥–Ω–∞–ª–∞)
        C_l_err = 0.1 * C_l + 50  # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞ –≥—Ä–µ—à–∫–∞
        
        return {
            'l_values': l_values,
            'C_l_TT': C_l,
            'C_l_TT_err': C_l_err,
            'description': '–°–∏–º—É–ª–∏—Ä–∞–Ω CMB TT power spectrum'
        }
    
    def _create_cmb_covariance_matrices(self):
        """–°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏ –∑–∞ CMB –¥–∞–Ω–Ω–∏"""
        
        # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        peak_errors = np.array([0.5, 2.0, 4.0])  # –ì—Ä–µ—à–∫–∏ –Ω–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        peak_cov = np.diag(peak_errors**2)
        
        # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —Å–ª–∞–±–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏—è –º–µ–∂–¥—É –ø–∏–∫–æ–≤–µ—Ç–µ
        correlation_strength = 0.2
        n_peaks = len(peak_errors)
        for i in range(n_peaks):
            for j in range(n_peaks):
                if i != j:
                    peak_cov[i, j] = correlation_strength * np.sqrt(peak_cov[i, i] * peak_cov[j, j])
        
        self.covariance_matrices['peak_positions'] = peak_cov
        
        # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ theta_s –∏ –¥—Ä—É–≥–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        base_params_cov = np.diag([0.0000031**2, 0.3**2, 0.26**2])  # theta_s, DA_star, rs_star (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ Planck –≥—Ä–µ—à–∫–∞)
        self.covariance_matrices['base_parameters'] = base_params_cov
        
        # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ power spectrum (diagonal approximation)
        power_spectrum_data = self.datasets['CMB_power_spectrum']
        C_l_err = power_spectrum_data['C_l_TT_err']
        power_spectrum_cov = np.diag(C_l_err**2)
        
        # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∫–æ—Ä–µ–ª–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å—ä—Å–µ–¥–Ω–∏ l-values
        n_l = len(C_l_err)
        for i in range(n_l - 1):
            correlation = 0.3 * np.sqrt(power_spectrum_cov[i, i] * power_spectrum_cov[i+1, i+1])
            power_spectrum_cov[i, i+1] = correlation
            power_spectrum_cov[i+1, i] = correlation
        
        self.covariance_matrices['power_spectrum'] = power_spectrum_cov
    
    def get_peak_positions(self) -> Dict:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ CMB –ø–∏–∫–æ–≤–µ—Ç–µ"""
        base_data = self.datasets['Planck_2018_base']
        
        return {
            'l_peaks': np.array([base_data['l_peak_1'], base_data['l_peak_2'], base_data['l_peak_3']]),
            'l_peaks_err': np.array([base_data['l_peak_1_err'], base_data['l_peak_2_err'], base_data['l_peak_3_err']]),
            'covariance': self.covariance_matrices['peak_positions']
        }
    
    def get_acoustic_scale(self) -> Dict:
        """–ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ –∞–∫—É—Å—Ç–∏—á–Ω–∞—Ç–∞ —Å–∫–∞–ª–∞"""
        base_data = self.datasets['Planck_2018_base']
        
        return {
            'theta_s': base_data['theta_s'],
            'theta_s_err': base_data['theta_s_err']
        }
    
    def summary(self):
        """–†–µ–∑—é–º–µ –Ω–∞ CMB –¥–∞–Ω–Ω–∏"""
        print("üåå CMB –ù–ê–ë–õ–Æ–î–ê–¢–ï–õ–ù–ò –î–ê–ù–ù–ò")
        print("=" * 50)
        
        for dataset_name, data in self.datasets.items():
            print(f"\n{dataset_name}:")
            print(f"  –û–ø–∏—Å–∞–Ω–∏–µ: {data['description']}")
            
            if 'theta_s' in data:
                print(f"  Œ∏_s = {data['theta_s']:.7f} ¬± {data['theta_s_err']:.7f}")
            
            if 'l_peak_1' in data:
                print(f"  l_peak_1 = {data['l_peak_1']:.1f} ¬± {data['l_peak_1_err']:.1f}")
                print(f"  l_peak_2 = {data['l_peak_2']:.1f} ¬± {data['l_peak_2_err']:.1f}")
                print(f"  l_peak_3 = {data['l_peak_3']:.1f} ¬± {data['l_peak_3_err']:.1f}")
            
            if 'DA_star' in data:
                print(f"  D_A(z*) = {data['DA_star']:.1f} ¬± {data['DA_star_err']:.1f} Mpc")
                print(f"  r_s(z*) = {data['rs_star']:.2f} ¬± {data['rs_star_err']:.2f} Mpc")
            
            if 'l_values' in data:
                print(f"  Power spectrum: {len(data['l_values'])} l-values")
                print(f"  l range: {data['l_values'][0]} - {data['l_values'][-1]}")


class LikelihoodFunctions:
    """
    Likelihood —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ BAO –∏ CMB –¥–∞–Ω–Ω–∏
    
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—è —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞:
    - BAO likelihood
    - CMB likelihood
    - –û–±–µ–¥–∏–Ω–µ–Ω–∞ likelihood
    - œá¬≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    """
    
    def __init__(self, bao_data: BAOObservationalData, cmb_data: CMBObservationalData):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ likelihood —Ñ—É–Ω–∫—Ü–∏–∏
        
        Args:
            bao_data: BAO –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
            cmb_data: CMB –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏
        """
        self.bao_data = bao_data
        self.cmb_data = cmb_data
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ likelihood —Ñ—É–Ω–∫—Ü–∏–∏")
    
    def bao_likelihood(self, model_predictions: Dict, dataset_names: List[str] = None) -> float:
        """
        BAO likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            dataset_names: –ò–º–µ–Ω–∞ –Ω–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–∏—Ç–µ –ø—Ä–æ—É—á–≤–∞–Ω–∏—è
            
        Returns:
            Log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        if dataset_names is None:
            dataset_names = list(self.bao_data.datasets.keys())
        
        total_log_likelihood = 0.0
        model_index = 0
        
        for dataset_name in dataset_names:
            if dataset_name not in self.bao_data.datasets:
                continue
                
            data = self.bao_data.datasets[dataset_name]
            
            # –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
            z_obs = data['redshifts']
            DV_rs_obs = data['DV_rs']
            DV_rs_err = data['DV_rs_err']
            n_points = len(z_obs)
            
            # –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            if 'DV_rs' in model_predictions:
                DV_rs_model_all = model_predictions['DV_rs']
                
                # –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä–æ—Ç–≤–µ—Ç–Ω–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ —Ç–æ–∑–∏ dataset
                if len(DV_rs_model_all) >= model_index + n_points:
                    DV_rs_model = DV_rs_model_all[model_index:model_index + n_points]
                    model_index += n_points
                    
                    # –†–µ–∑–∏–¥—É–∞–ª–∏
                    residuals = DV_rs_obs - DV_rs_model
                    
                    # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
                    cov_matrix = self.bao_data.covariance_matrices[f'{dataset_name}_DV']
                    
                    # œá¬≤ –∏–∑—á–∏—Å–ª–µ–Ω–∏–µ
                    chi2 = np.dot(residuals, np.dot(np.linalg.inv(cov_matrix), residuals))
                    
                    # Log-likelihood
                    log_likelihood = -0.5 * chi2
                    total_log_likelihood += log_likelihood
                else:
                    logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ {dataset_name}")
        
        return total_log_likelihood
    
    def cmb_likelihood(self, model_predictions: Dict) -> float:
        """
        CMB likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            
        Returns:
            Log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        total_log_likelihood = 0.0
        
        # Likelihood –∑–∞ –ø–æ–∑–∏—Ü–∏–∏—Ç–µ –Ω–∞ –ø–∏–∫–æ–≤–µ—Ç–µ
        if 'l_peaks' in model_predictions:
            peak_data = self.cmb_data.get_peak_positions()
            
            l_peaks_obs = peak_data['l_peaks']
            l_peaks_model = model_predictions['l_peaks']
            cov_matrix = peak_data['covariance']
            
            residuals = l_peaks_obs - l_peaks_model
            chi2 = np.dot(residuals, np.dot(np.linalg.inv(cov_matrix), residuals))
            
            log_likelihood = -0.5 * chi2
            total_log_likelihood += log_likelihood
        
        # Likelihood –∑–∞ –∞–∫—É—Å—Ç–∏—á–Ω–∞—Ç–∞ —Å–∫–∞–ª–∞
        if 'theta_s' in model_predictions:
            acoustic_data = self.cmb_data.get_acoustic_scale()
            
            theta_s_obs = acoustic_data['theta_s']
            theta_s_model = model_predictions['theta_s']
            theta_s_err = acoustic_data['theta_s_err']
            
            chi2 = ((theta_s_obs - theta_s_model) / theta_s_err)**2
            log_likelihood = -0.5 * chi2
            total_log_likelihood += log_likelihood
        
        return total_log_likelihood
    
    def combined_likelihood(self, model_predictions: Dict, 
                          bao_weight: float = 1.0, cmb_weight: float = 1.0) -> float:
        """
        –û–±–µ–¥–∏–Ω–µ–Ω–∞ BAO + CMB likelihood —Ñ—É–Ω–∫—Ü–∏—è
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            bao_weight: –¢–µ–≥–ª–æ –Ω–∞ BAO –¥–∞–Ω–Ω–∏—Ç–µ
            cmb_weight: –¢–µ–≥–ª–æ –Ω–∞ CMB –¥–∞–Ω–Ω–∏—Ç–µ
            
        Returns:
            –û–±–µ–¥–∏–Ω–µ–Ω–∞ log-likelihood —Å—Ç–æ–π–Ω–æ—Å—Ç
        """
        bao_loglike = self.bao_likelihood(model_predictions)
        cmb_loglike = self.cmb_likelihood(model_predictions)
        
        total_loglike = bao_weight * bao_loglike + cmb_weight * cmb_loglike
        
        return total_loglike
    
    def chi_squared_analysis(self, model_predictions: Dict) -> Dict:
        """
        –ü–æ–¥—Ä–æ–±–µ–Ω œá¬≤ –∞–Ω–∞–ª–∏–∑
        
        Args:
            model_predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
            
        Returns:
            –†–µ—á–Ω–∏–∫ —Å œá¬≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        """
        results = {}
        
        # BAO œá¬≤
        bao_loglike = self.bao_likelihood(model_predictions)
        bao_chi2 = -2 * bao_loglike
        results['bao_chi2'] = bao_chi2
        
        # CMB œá¬≤
        cmb_loglike = self.cmb_likelihood(model_predictions)
        cmb_chi2 = -2 * cmb_loglike
        results['cmb_chi2'] = cmb_chi2
        
        # –û–±–µ–¥–∏–Ω–µ–Ω œá¬≤
        combined_chi2 = bao_chi2 + cmb_chi2
        results['combined_chi2'] = combined_chi2
        
        # –°—Ç–µ–ø–µ–Ω–∏ –Ω–∞ —Å–≤–æ–±–æ–¥–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª–Ω–æ)
        results['dof_bao'] = len(self.bao_data.get_combined_data()['redshifts'])
        results['dof_cmb'] = 4  # theta_s + 3 –ø–∏–∫–∞
        results['dof_combined'] = results['dof_bao'] + results['dof_cmb']
        
        # Reduciran œá¬≤
        results['reduced_chi2_bao'] = bao_chi2 / results['dof_bao']
        results['reduced_chi2_cmb'] = cmb_chi2 / results['dof_cmb']
        results['reduced_chi2_combined'] = combined_chi2 / results['dof_combined']
        
        return results


def create_bao_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
    """
    –°—ä–∑–¥–∞–≤–∞ BAO –¥–∞–Ω–Ω–∏ —Å –ø—ä–ª–Ω–∏ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏ –º–∞—Ç—Ä–∏—Ü–∏
    
    Returns:
        z_values: –†–µ–¥shift —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
        measurements: DV/rs —Å—Ç–æ–π–Ω–æ—Å—Ç–∏
        errors: –î–∏–∞–≥–æ–Ω–∞–ª–Ω–∏ –≥—Ä–µ—à–∫–∏ (–∑–∞ backwards compatibility) 
        covariance_matrix: –ü—ä–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
    """
    
    logger.info("–ó–∞—Ä–µ–¥–µ–Ω–∏ BAO –¥–∞–Ω–Ω–∏ –æ—Ç BOSS/eBOSS/6dFGS/WiggleZ")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∏—Ç–µ –º–∞—Ç—Ä–∏—Ü–∏
    bao_cov = BAOCovarianceMatrices()
    
    # –û—Å–Ω–æ–≤–Ω–∏ BAO –¥–∞–Ω–Ω–∏ (—Å—ä–≥–ª–∞—Å—É–≤–∞–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏)
    bao_data = [
        # BOSS DR12 Consensus
        (0.38, 15.12, 0.38),
        (0.51, 19.75, 0.45),
        (0.61, 21.40, 0.51),
        
        # eBOSS DR16
        (0.70, 22.08, 0.54),
        (0.85, 23.50, 0.64),
        (1.48, 24.92, 0.75),
        
        # 6dFGS
        (0.106, 4.57, 0.29),
        
        # WiggleZ
        (0.44, 17.16, 0.85),
        (0.60, 22.21, 1.07),
        (0.73, 25.16, 1.31),
    ]
    
    z_values = np.array([data[0] for data in bao_data])
    measurements = np.array([data[1] for data in bao_data])
    errors = np.array([data[2] for data in bao_data])
    
    # –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –ø—ä–ª–Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞
    try:
        covariance_matrix = bao_cov.get_full_covariance_matrix('Combined')
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∞ –ø—ä–ª–Ω–∞ BAO –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ {covariance_matrix.shape}")
        logger.info(f"Condition number: {np.linalg.cond(covariance_matrix):.2e}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞
        validation_results = bao_cov.validate_covariance_matrix('Combined')
        if not validation_results['is_positive_definite']:
            logger.warning("–ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—Ç–∞ –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ - –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞")
            covariance_matrix = None
        elif validation_results['condition_number'] > 1e12:
            logger.warning("–ö–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—Ç–∞ –º–∞—Ç—Ä–∏—Ü–∞ –µ –ª–æ—à–æ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–∞–Ω–∞ - –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–Ω–∞")
            covariance_matrix = None
            
    except Exception as e:
        logger.error(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞: {e}")
        covariance_matrix = None
    
    return z_values, measurements, errors, covariance_matrix


def test_observational_data():
    """–¢–µ—Å—Ç –Ω–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏"""
    
    print("üß™ –¢–ï–°–¢ –ù–ê –ù–ê–ë–õ–Æ–î–ê–¢–ï–õ–ù–ò–¢–ï –î–ê–ù–ù–ò")
    print("=" * 70)
    
    # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –æ–±–µ–∫—Ç–∏
    bao_data = BAOObservationalData()
    cmb_data = CMBObservationalData()
    
    # –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —Ä–µ–∑—é–º–µ—Ç–∞
    bao_data.summary()
    cmb_data.summary()
    
    # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ likelihood —Ñ—É–Ω–∫—Ü–∏–∏
    likelihood = LikelihoodFunctions(bao_data, cmb_data)
    
    # –¢–µ—Å—Ç–æ–≤–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–æ–¥–µ–ª–∞
    # –¢—Ä—è–±–≤–∞ –¥–∞ —Å—ä–æ—Ç–≤–µ—Ç—Å—Ç–≤–∞—Ç –Ω–∞ —Ä–µ–¥–∞: BOSS_DR12 (3), eBOSS_DR16 (3), 6dFGS (1), WiggleZ (3)
    test_predictions = {
        'DV_rs': np.array([1500, 1950, 2100,  # BOSS_DR12
                          2200, 2300, 2450,  # eBOSS_DR16
                          450,               # 6dFGS
                          1700, 2200, 2500]),  # WiggleZ
        'l_peaks': np.array([220, 546, 800]),
        'theta_s': 0.0104092
    }
    
    # –¢–µ—Å—Ç –Ω–∞ œá¬≤ –∞–Ω–∞–ª–∏–∑
    print(f"\nüîç œá¬≤ –ê–ù–ê–õ–ò–ó:")
    chi2_results = likelihood.chi_squared_analysis(test_predictions)
    
    for key, value in chi2_results.items():
        print(f"  {key}: {value:.2f}")
    
    # –¢–µ—Å—Ç –Ω–∞ likelihood —Ñ—É–Ω–∫—Ü–∏–∏
    print(f"\nüìä LIKELIHOOD –§–£–ù–ö–¶–ò–ò:")
    bao_loglike = likelihood.bao_likelihood(test_predictions)
    cmb_loglike = likelihood.cmb_likelihood(test_predictions)
    combined_loglike = likelihood.combined_likelihood(test_predictions)
    
    print(f"  BAO log-likelihood: {bao_loglike:.2f}")
    print(f"  CMB log-likelihood: {cmb_loglike:.2f}")
    print(f"  Combined log-likelihood: {combined_loglike:.2f}")
    
    print("\n‚úÖ –¢–µ—Å—Ç—ä—Ç –Ω–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏ –∑–∞–≤—ä—Ä—à–∏ —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    test_observational_data() 